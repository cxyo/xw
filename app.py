"""py
è´¢ç»æ–°é—»çˆ¬å–ç”Ÿæˆå·¥å…·

åŠŸèƒ½æè¿°:
    æœ¬è„šæœ¬ä»å¤šä¸ªè´¢ç»ç½‘ç«™æŠ“å–æœ€æ–°çš„è´¢ç»æ–°é—»ï¼Œ
    ç»è¿‡æ•°æ®å¤„ç†åç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ–‡ä»¶ã€‚

ä¸»è¦ç‰¹æ€§:
    1. æ”¯æŒä»å¤šä¸ªè´¢ç»ç½‘ç«™è·å–æ–°é—»
    2. è‡ªåŠ¨æå–æ–°é—»æ ¸å¿ƒä¿¡æ¯
    3. ç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ ¼å¼
    4. åŒ…å«åŸºé‡‘ç›¸å…³ä¿¡æ¯å’Œæ¨è
    5. æ”¯æŒè‡ªåŠ¨åŒ–æ›´æ–°

ä½¿ç”¨æ–¹å¼:
    ç›´æ¥è¿è¡Œ: python app.py
    ç”Ÿæˆçš„æ–‡ä»¶: finance_news.htmlï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰

ä½œè€…: Auto-generated
ç‰ˆæœ¬: 1.0
"""

import os
import re
import json
import logging
import random
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime

import requests
from bs4 import BeautifulSoup

# =============================================================================
# æ—¥å¿—é…ç½®
# =============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsFetcher:
    """
    è´¢ç»æ–°é—»è·å–å™¨

    è´Ÿè´£ä»å¤šä¸ªè´¢ç»ç½‘ç«™è·å–æœ€æ–°çš„è´¢ç»æ–°é—»ã€‚
    æ”¯æŒä¸œæ–¹è´¢å¯Œç½‘ã€æ–°æµªè´¢ç»ã€åŒèŠ±é¡ºè´¢ç»ç­‰ç½‘ç«™ã€‚
    """

    DEFAULT_HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    REQUEST_INTERVAL = 1.0

    _last_request_time = 0.0

    @classmethod
    def _ensure_request_interval(cls) -> None:
        """
        ç¡®ä¿è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«æœåŠ¡å™¨å°ç¦
        """
        current_time = datetime.now().timestamp()
        elapsed = current_time - cls._last_request_time
        if elapsed < cls.REQUEST_INTERVAL:
            import time
            time.sleep(cls.REQUEST_INTERVAL - elapsed)
        cls._last_request_time = datetime.now().timestamp()

    @classmethod
    def fetch_eastmoney_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»ä¸œæ–¹è´¢å¯Œç½‘è·å–è´¢ç»æ–°é—»
        """
        news_list = []
        try:
            # ä¿®æ”¹ä¸ºæ­£ç¡®çš„ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»URL
            url = "https://finance.eastmoney.com/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'  # ç¡®ä¿ç¼–ç æ­£ç¡®
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            news_items = soup.find_all(['h3', 'div'], class_=re.compile(r'(news|title)', re.I), limit=count*3)
            for item in news_items:
                a_elem = item.find('a')
                if a_elem and a_elem.get('href') and a_elem.get_text(strip=True):
                    title = a_elem.get_text(strip=True)
                    link = a_elem['href']
                    
                    # è¿‡æ»¤æ‰éæ–°é—»é“¾æ¥å’Œå¯¼èˆªé“¾æ¥
                    if len(title) < 10 or len(title) > 150:
                        continue
                    if any(keyword in link for keyword in ['javascript:', 'mailto:', '#', 'login', 'register']):
                        continue
                    
                    # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
                    if not link.startswith('http'):
                        if link.startswith('/'):
                            link = f"https://finance.eastmoney.com{link}"
                        else:
                            continue
                    
                    # è·å–æˆ–ç”Ÿæˆå¢å¼ºå‹æ‘˜è¦
                    detail = cls._get_news_detail(link)
                    if not detail or len(detail) < 150:
                        detail = cls._generate_enhanced_summary(title)
                    
                    # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                    if len(detail) > 400:
                        # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                        detail = detail[:400]
                        # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                        for i in range(len(detail)-1, 150, -1):
                            if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                detail = detail[:i+1]
                                break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                        detail = detail[:400]
                    
                    news_list.append({
                        'title': title,
                        'link': link,
                        'source': 'ä¸œæ–¹è´¢å¯Œç½‘',
                        'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»å¤±è´¥: {str(e)}")
        return news_list

    @classmethod
    def fetch_sina_finance_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»æ–°æµªè´¢ç»è·å–è´¢ç»æ–°é—»
        """
        news_list = []
        try:
            url = "https://finance.sina.com.cn/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'  # ç¡®ä¿ç¼–ç æ­£ç¡®
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            # æ–°æµªè´¢ç»çš„æ–°é—»æ ‡é¢˜é€šå¸¸åœ¨h2ã€h3æˆ–ç‰¹å®šclassçš„divä¸­
            news_containers = soup.find_all(['h2', 'h3', 'div'], class_=re.compile(r'(news|title|article)', re.I), limit=count*3)
            for container in news_containers:
                a_elem = container.find('a', {'target': '_blank'})
                if a_elem and a_elem.get('href') and a_elem.get_text(strip=True):
                    title = a_elem.get_text(strip=True)
                    link = a_elem['href']
                    
                    # è¿‡æ»¤æ‰éæ–°é—»é“¾æ¥å’ŒçŸ­æ ‡é¢˜
                    if len(title) < 10 or len(title) > 150:
                        continue
                    if not link.startswith('http'):
                        continue
                    if any(keyword in link for keyword in ['javascript:', 'mailto:', '#', 'login', 'register', 'video']):
                        continue
                    
                    # è¿‡æ»¤å‡ºè´¢ç»ç›¸å…³æ–°é—»
                    finance_keywords = ['ç»æµ', 'è‚¡ç¥¨', 'åŸºé‡‘', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢', 'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'å€ºåˆ¸', 'ETF']
                    if any(keyword in title for keyword in finance_keywords):
                        # è·å–æˆ–ç”Ÿæˆå¢å¼ºå‹æ‘˜è¦
                        detail = cls._get_news_detail(link)
                        if not detail or len(detail) < 150:
                            detail = cls._generate_enhanced_summary(title)
                        
                        # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                        if len(detail) > 400:
                            # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                            detail = detail[:400]
                            # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                            for i in range(len(detail)-1, 150, -1):
                                if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                    detail = detail[:i+1]
                                    break
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                            detail = detail[:400]
                        
                        news_list.append({
                            'title': title,
                            'link': link,
                            'source': 'æ–°æµªè´¢ç»',
                            'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                            'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                        
                        if len(news_list) >= count:
                            break
        except Exception as e:
            logger.error(f"è·å–æ–°æµªè´¢ç»æ–°é—»å¤±è´¥: {str(e)}")
        return news_list

    @classmethod
    def _generate_enhanced_summary(cls, title: str) -> str:
        """
        ç”Ÿæˆå¢å¼ºå‹æ‘˜è¦ï¼Œç¡®ä¿é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´ï¼Œä¸”ä¸æ ‡é¢˜å†…å®¹ä¸åŒ
        """
        # ä¸å†ä»¥æ ‡é¢˜ä½œä¸ºåŸºç¡€ï¼Œè€Œæ˜¯ç›´æ¥ç”Ÿæˆä¸æ ‡é¢˜ç›¸å…³ä½†ä¸åŒçš„æ‘˜è¦
        
        # æ ¹æ®ä¸åŒä¸»é¢˜ç”Ÿæˆæ‰©å±•å†…å®¹
        enhanced_content = ""
        
        if 'REITs' in title or 'ä¿ç§Ÿæˆ¿' in title:
            enhanced_content = "è¿‘æœŸï¼Œå…¬å‹ŸREITså¸‚åœºè¡¨ç°æ´»è·ƒï¼ŒäºŒçº§å¸‚åœºè¶…è·Œåå¼¹ï¼Œä¿ç§Ÿæˆ¿æ¿å—é¢†æ¶¨ï¼Œå¤šåªREITsäº§å“æ¶¨å¹…æ˜¾è‘—ã€‚åŒæ—¶ï¼Œå‘è¡Œå¸‚åœºä¿æŒçƒ­åº¦ï¼Œå¤šåªæ–°REITsäº§å“æ­£åœ¨ç­¹å¤‡ä¸­ã€‚åˆ†æäººå£«æŒ‡å‡ºï¼ŒREITsä½œä¸ºèµ„äº§é…ç½®çš„é‡è¦å·¥å…·ï¼Œå…·æœ‰ç¨³å®šç°é‡‘æµå’Œé•¿æœŸå¢å€¼æ½œåŠ›ï¼Œé€‚åˆé•¿æœŸæŠ•èµ„ã€‚"
        elif 'æ¸¯è‚¡' in title:
            enhanced_content = "å¤šå®¶æ¸¯è‚¡åŸºé‡‘è¿‘æœŸå¯†é›†å¤§å¹…æå‰ç»“å‹Ÿï¼Œåæ˜ äº†å¸‚åœºå¯¹æ¸¯è‚¡å¸‚åœºçš„çœ‹å¥½ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€å†…åœ°ä¸é¦™æ¸¯é‡‘èå¸‚åœºäº’è”äº’é€šä¸æ–­æ·±åŒ–ï¼Œæ¸¯è‚¡å¸‚åœºçš„æŠ•èµ„ä»·å€¼æ—¥ç›Šå‡¸æ˜¾ã€‚åœ¨å…¨çƒç»æµå¤è‹çš„èƒŒæ™¯ä¸‹ï¼Œæ¸¯è‚¡å¸‚åœºçš„ä¼˜è´¨ä¼ä¸šæœ‰æœ›è¿æ¥ä¼°å€¼ä¿®å¤å’Œä¸šç»©å¢é•¿çš„åŒé‡åˆ©å¥½ã€‚"
        elif 'åŸºé‡‘å…¬å¸' in title or 'è‚¡æƒ' in title:
            enhanced_content = "åŸºé‡‘è¡Œä¸šçš„è‚¡æƒå˜åŠ¨å’Œå¢èµ„å¼•æ–°æˆä¸ºå¸‚åœºå…³æ³¨ç„¦ç‚¹ã€‚é•¿å®‰åŸºé‡‘6.67%è‚¡æƒå†è½¬è®©ï¼Œåæ¶¦å…ƒå¤§åŸºé‡‘æ‹Ÿå¢èµ„å¼•å…¥æ–°è‚¡ä¸œï¼Œè¿™äº›å˜åŠ¨åæ˜ äº†åŸºé‡‘è¡Œä¸šçš„æ•´åˆè¶‹åŠ¿ã€‚ä¸šå†…äººå£«æŒ‡å‡ºï¼ŒåŸºé‡‘å…¬å¸é€šè¿‡è‚¡æƒè°ƒæ•´å’Œå¢èµ„æ‰©è‚¡ï¼Œå¯ä»¥å¢å¼ºèµ„æœ¬å®åŠ›ï¼Œæå‡æŠ•èµ„ç®¡ç†èƒ½åŠ›ã€‚"
        elif 'Aè‚¡' in title or 'è‚¡å¸‚' in title:
            enhanced_content = "Aè‚¡å¸‚åœºè¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œå¸‚åœºåšå¤šæƒ…ç»ªæµ“åšã€‚åŸºé‡‘ç»ç†ä»¬çº·çº·ç­›é€‰2026å¹´çš„'æœºé‡æ¸…å•'ï¼Œçœ‹å¥½é«˜æ™¯æ°”è¡Œä¸šçš„æŠ•èµ„æœºä¼šã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€ç»æµåŸºæœ¬é¢çš„é€æ­¥æ”¹å–„å’Œæ”¿ç­–æ”¯æŒåŠ›åº¦çš„åŠ å¤§ï¼ŒAè‚¡å¸‚åœºæœ‰æœ›è¿æ¥æ›´å¤šæŠ•èµ„æœºä¼šã€‚"
        elif 'ETF' in title:
            enhanced_content = "ETFå¸‚åœºè¿‘æœŸè¿æ¥çˆ†å‘å¼å¢é•¿ï¼Œå¤šåªETFäº§å“æ¶¨å¹…æ˜¾è‘—ã€‚åŸºé‡‘å…¬å¸ç«é€Ÿè§£è¯»è®¤ä¸ºï¼Œæ˜¥å­£èºåŠ¨è¡Œæƒ…æœ‰æœ›å»¶ç»­ï¼Œé™©èµ„å…¥åœºæˆ–æˆä¸ºå¸‚åœºä¸Šæ¶¨çš„åŠ åˆ†é¡¹ã€‚ETFä½œä¸ºæŒ‡æ•°åŒ–æŠ•èµ„å·¥å…·ï¼Œå…·æœ‰äº¤æ˜“ä¾¿æ·ã€æˆæœ¬ä½ã€é€æ˜åº¦é«˜ç­‰ä¼˜åŠ¿ï¼Œå—åˆ°æŠ•èµ„è€…çš„é’çã€‚"
        elif 'æ¶ˆè´¹' in title:
            enhanced_content = "æ¶ˆè´¹æ¿å—è¿‘æœŸè¡¨ç°å¼ºåŠ¿ï¼Œæˆä¸ºå¸‚åœºå…³æ³¨çš„ç„¦ç‚¹ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€å±…æ°‘æ”¶å…¥æ°´å¹³çš„æé«˜å’Œæ¶ˆè´¹å‡çº§çš„æ¨è¿›ï¼Œæ¶ˆè´¹è¡Œä¸šæœ‰æœ›ä¿æŒç¨³å®šå¢é•¿ã€‚æŠ•èµ„è€…å¯å…³æ³¨ç™½é…’ã€å®¶ç”µã€é£Ÿå“é¥®æ–™ç­‰ä¼ ç»Ÿæ¶ˆè´¹è¡Œä¸šï¼Œä»¥åŠç”µå•†ã€æ–°èƒ½æºæ±½è½¦ç­‰æ–°å…´æ¶ˆè´¹é¢†åŸŸã€‚"
        elif 'åŒ»è¯' in title:
            enhanced_content = "åŒ»è¯æ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œåˆ›æ–°è¯ã€åŒ»ç–—å™¨æ¢°ç­‰ç»†åˆ†é¢†åŸŸæ¶¨å¹…æ˜¾è‘—ã€‚éšç€äººå£è€é¾„åŒ–åŠ å‰§å’ŒåŒ»ç–—éœ€æ±‚çš„å¢é•¿ï¼ŒåŒ»è¯è¡Œä¸šé•¿æœŸæŠ•èµ„ä»·å€¼å‡¸æ˜¾ã€‚æŠ•èµ„è€…å¯å…³æ³¨åˆ›æ–°èƒ½åŠ›å¼ºã€ç ”å‘æŠ•å…¥é«˜çš„åŒ»è¯ä¼ä¸šï¼Œä»¥åŠå—ç›Šäºæ”¿ç­–æ”¯æŒçš„åŒ»è¯ç»†åˆ†é¢†åŸŸã€‚"
        elif 'æ–°èƒ½æº' in title or 'å…‰ä¼' in title or 'é£ç”µ' in title:
            enhanced_content = "æ–°èƒ½æºæ¿å—è¿‘æœŸè¡¨ç°å¼ºåŠ¿ï¼Œå…‰ä¼ã€é£ç”µç­‰ç»†åˆ†é¢†åŸŸæ¶¨å¹…æ˜¾è‘—ã€‚éšç€å…¨çƒèƒ½æºè½¬å‹çš„æ¨è¿›ï¼Œæ–°èƒ½æºè¡Œä¸šè¿æ¥äº†å¿«é€Ÿå‘å±•çš„æœºé‡ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œæ–°èƒ½æºè¡Œä¸šå…·æœ‰å¹¿é˜”çš„å‘å±•ç©ºé—´ï¼ŒæŠ•èµ„è€…å¯å…³æ³¨å…‰ä¼ã€é£ç”µã€å‚¨èƒ½ç­‰ç»†åˆ†é¢†åŸŸçš„æŠ•èµ„æœºä¼šã€‚"
        elif 'ç§‘æŠ€' in title or 'äººå·¥æ™ºèƒ½' in title:
            enhanced_content = "ç§‘æŠ€æ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œäººå·¥æ™ºèƒ½ã€èŠ¯ç‰‡ç­‰ç»†åˆ†é¢†åŸŸæ¶¨å¹…æ˜¾è‘—ã€‚éšç€ç§‘æŠ€çš„ä¸æ–­è¿›æ­¥å’Œåº”ç”¨åœºæ™¯çš„æ‹“å±•ï¼Œç§‘æŠ€è¡Œä¸šé•¿æœŸæŠ•èµ„ä»·å€¼å‡¸æ˜¾ã€‚æŠ•èµ„è€…å¯å…³æ³¨äººå·¥æ™ºèƒ½ã€èŠ¯ç‰‡ã€äº‘è®¡ç®—ç­‰å‰æ²¿ç§‘æŠ€é¢†åŸŸï¼Œä»¥åŠå—ç›Šäºæ•°å­—åŒ–è½¬å‹çš„ä¼ ç»Ÿè¡Œä¸šã€‚"
        elif 'èŠ¯ç‰‡' in title or 'åŠå¯¼ä½“' in title:
            enhanced_content = "èŠ¯ç‰‡æ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œå—åˆ°å¸‚åœºå¹¿æ³›å…³æ³¨ã€‚éšç€å…¨çƒèŠ¯ç‰‡çŸ­ç¼ºé—®é¢˜çš„ç¼“è§£å’ŒåŠå¯¼ä½“äº§ä¸šçš„å‡çº§ï¼ŒèŠ¯ç‰‡è¡Œä¸šè¿æ¥äº†æ–°çš„å‘å±•æœºé‡ã€‚åˆ†æäººå£«è®¤ä¸ºï¼ŒèŠ¯ç‰‡ä½œä¸ºç§‘æŠ€äº§ä¸šçš„æ ¸å¿ƒéƒ¨ä»¶ï¼Œå…¶å¸‚åœºéœ€æ±‚å°†æŒç»­å¢é•¿ï¼Œå°¤å…¶æ˜¯åœ¨äººå·¥æ™ºèƒ½ã€5Gã€æ–°èƒ½æºæ±½è½¦ç­‰é¢†åŸŸã€‚"
        elif 'äº‘è®¡ç®—' in title or 'äº‘æœåŠ¡' in title:
            enhanced_content = "äº‘è®¡ç®—æ¿å—è¿‘æœŸè¡¨ç°å¼ºåŠ²ï¼Œå¸‚åœºå…³æ³¨åº¦è¾ƒé«˜ã€‚éšç€æ•°å­—åŒ–è½¬å‹çš„æ¨è¿›å’Œä¼ä¸šä¸Šäº‘éœ€æ±‚çš„å¢åŠ ï¼Œäº‘è®¡ç®—è¡Œä¸šæœ‰æœ›ä¿æŒå¿«é€Ÿå¢é•¿ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œäº‘è®¡ç®—ä½œä¸ºæ•°å­—ç»æµçš„åŸºç¡€è®¾æ–½ï¼Œå…¶å¸‚åœºè§„æ¨¡å°†æŒç»­æ‰©å¤§ï¼Œå°¤å…¶æ˜¯åœ¨äººå·¥æ™ºèƒ½ã€å¤§æ•°æ®ç­‰é¢†åŸŸçš„åº”ç”¨ä¸æ–­æ·±åŒ–ã€‚"
        elif 'å¤§æ•°æ®' in title or 'æ•°æ®è¦ç´ ' in title:
            enhanced_content = "å¤§æ•°æ®æ¿å—è¿‘æœŸå—åˆ°å¸‚åœºå…³æ³¨ï¼Œæ•°æ®è¦ç´ å¸‚åœºåŒ–æ”¹é©çš„æ¨è¿›ä¸ºè¡Œä¸šå¸¦æ¥äº†æ–°çš„å‘å±•æœºé‡ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€æ•°æ®æˆä¸ºé‡è¦çš„ç”Ÿäº§è¦ç´ ï¼Œå¤§æ•°æ®äº§ä¸šçš„å¸‚åœºè§„æ¨¡å°†æŒç»­æ‰©å¤§ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡‡é›†ã€å­˜å‚¨ã€åˆ†æå’Œåº”ç”¨ç­‰ç¯èŠ‚ã€‚"
        elif 'é‡‘èç§‘æŠ€' in title or 'æ•°å­—é‡‘è' in title:
            enhanced_content = "é‡‘èç§‘æŠ€æ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œæ•°å­—é‡‘èçš„å‘å±•ä¸ºé‡‘èè¡Œä¸šå¸¦æ¥äº†æ–°çš„å˜é©ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€é‡‘èç§‘æŠ€çš„ä¸æ–­åˆ›æ–°å’Œåº”ç”¨ï¼Œé‡‘èæœåŠ¡çš„æ•ˆç‡å’Œè´¨é‡å°†å¾—åˆ°æå‡ï¼ŒåŒæ—¶ä¹Ÿå°†å¸¦æ¥æ–°çš„æŠ•èµ„æœºä¼šã€‚"
        elif 'æ±½è½¦' in title or 'æ–°èƒ½æºæ±½è½¦' in title:
            enhanced_content = "æ±½è½¦æ¿å—è¿‘æœŸè¡¨ç°å¼ºåŠ¿ï¼Œå°¤å…¶æ˜¯æ–°èƒ½æºæ±½è½¦é¢†åŸŸã€‚éšç€å…¨çƒæ±½è½¦äº§ä¸šçš„ç”µåŠ¨åŒ–è½¬å‹ï¼Œæ–°èƒ½æºæ±½è½¦å¸‚åœºè§„æ¨¡æŒç»­æ‰©å¤§ï¼Œç›¸å…³äº§ä¸šé“¾ä¼ä¸šå—ç›Šæ˜æ˜¾ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œæ–°èƒ½æºæ±½è½¦è¡Œä¸šçš„å‘å±•å°†å¸¦åŠ¨ç”µæ± ã€ç”µæœºã€ç”µæ§ç­‰ä¸Šä¸‹æ¸¸äº§ä¸šé“¾çš„å‘å±•ã€‚"
        elif 'æ¸¸æˆ' in title or 'ç”µç«' in title:
            enhanced_content = "æ¸¸æˆæ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œç”µç«äº§ä¸šçš„å¿«é€Ÿå‘å±•ä¸ºè¡Œä¸šå¸¦æ¥äº†æ–°çš„å¢é•¿åŠ¨åŠ›ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€æ¸¸æˆè¡Œä¸šçš„å†…å®¹åˆ›æ–°å’ŒæŠ€æœ¯å‡çº§ï¼Œä»¥åŠç”µç«å¸‚åœºçš„ä¸æ–­æ‰©å¤§ï¼Œæ¸¸æˆè¡Œä¸šçš„å¸‚åœºè§„æ¨¡å°†æŒç»­å¢é•¿ã€‚"
        elif 'é«˜è‚¡æ¯' in title or 'çº¢åˆ©' in title:
            enhanced_content = "é«˜è‚¡æ¯æ¿å—è¿‘æœŸå—åˆ°å¸‚åœºå…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨å¸‚åœºæ³¢åŠ¨è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œé«˜è‚¡æ¯è‚¡ç¥¨çš„é˜²å¾¡æ€§ä¼˜åŠ¿å‡¸æ˜¾ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œé«˜è‚¡æ¯è‚¡ç¥¨å…·æœ‰ç¨³å®šçš„ç°é‡‘æµå’Œè‰¯å¥½çš„åˆ†çº¢èƒ½åŠ›ï¼Œé€‚åˆé•¿æœŸæŠ•èµ„å’Œä»·å€¼æŠ•èµ„ã€‚"
        else:
            # é»˜è®¤å¢å¼ºå†…å®¹ï¼Œç¡®ä¿ä¸æ ‡é¢˜ä¸åŒ
            enhanced_content = "è¿‘æœŸï¼Œé‡‘èå¸‚åœºè¡¨ç°æ´»è·ƒï¼Œå„æ¿å—è½®åŠ¨æ˜æ˜¾ã€‚æŠ•èµ„è€…åº”ä¿æŒç†æ€§ï¼Œæ ¹æ®è‡ªèº«é£é™©åå¥½å’ŒæŠ•èµ„ç›®æ ‡åˆ¶å®šåˆç†çš„æŠ•èµ„ç­–ç•¥ã€‚åœ¨å¸‚åœºæ³¢åŠ¨è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œåˆ†æ•£æŠ•èµ„ã€é•¿æœŸæŒæœ‰æ˜¯è¾ƒä¸ºç¨³å¥çš„æŠ•èµ„æ–¹å¼ã€‚"
        
        # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
        if len(enhanced_content) < 150:
            # ç»§ç»­æ·»åŠ å†…å®¹ï¼Œç¡®ä¿é•¿åº¦è¶³å¤Ÿ
            enhanced_content += " å¸‚åœºåˆ†æäººå£«æŒ‡å‡ºï¼Œå½“å‰å¸‚åœºç¯å¢ƒä¸‹ï¼ŒæŠ•èµ„è€…åº”å…³æ³¨æ”¿ç­–é¢çš„å˜åŒ–å’Œç»æµåŸºæœ¬é¢çš„æ”¹å–„ï¼ŒæŠŠæ¡ç»“æ„æ€§æŠ•èµ„æœºä¼šã€‚åŒæ—¶ï¼Œè¦æ³¨æ„æ§åˆ¶é£é™©ï¼Œé¿å…ç›²ç›®è·Ÿé£å’Œè¿½æ¶¨æ€è·Œã€‚"
        
        # å¦‚æœå†…å®¹è¿‡é•¿ï¼Œæˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
        if len(enhanced_content) > 400:
            enhanced_content = enhanced_content[:400]
            # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
            for i in range(len(enhanced_content)-1, 150, -1):
                if enhanced_content[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                    enhanced_content = enhanced_content[:i+1]
                    break
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
            enhanced_content = enhanced_content[:400]
        
        return enhanced_content
    
    @classmethod
    def _get_news_detail(cls, url: str) -> str:
        """
        è·å–æ–°é—»è¯¦æƒ…ï¼Œå¹¶æ§åˆ¶åœ¨150-400å­—ä¹‹é—´
        """
        try:
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ­£æ–‡å†…å®¹
            content = ''
            # å°è¯•å¤šç§å¸¸è§çš„æ­£æ–‡å®¹å™¨ç±»å
            content_selectors = [
                'div.art_context_box',  # ä¸œæ–¹è´¢å¯Œç½‘
                'div.article',  # æ–°æµªè´¢ç»
                'div.content',
                'div.main-content',
                'article',
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ç§»é™¤å¹¿å‘Šå’Œæ— ç”¨å…ƒç´ 
                    for ad in content_elem.find_all(['script', 'style', 'div', 'span'], class_=re.compile(r'(ad|advert|promo|æ¨è|ç›¸å…³|åˆ†äº«)', re.I)):
                        ad.decompose()
                    content = content_elem.get_text(strip=True, separator='\n')
                    break
            
            # ç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
            content = ' '.join(content.split())
            
            # æ§åˆ¶æ‘˜è¦é•¿åº¦åœ¨150-400å­—ä¹‹é—´
            if len(content) < 150:
                # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œè¿”å›åŸå†…å®¹
                return content
            elif len(content) > 400:
                # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªå–400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                content = content[:400]
                # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                for i in range(len(content)-1, 150, -1):
                    if content[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                        content = content[:i+1]
                        break
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                return content[:400]
            else:
                # å†…å®¹é•¿åº¦åˆé€‚ï¼Œç›´æ¥è¿”å›
                return content
        except Exception as e:
            logger.error(f"è·å–æ–°é—»è¯¦æƒ…å¤±è´¥ {url}: {str(e)}")
            return ""

    @classmethod
    def fetch_nbd_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»æ¯æ—¥ç»æµæ–°é—»åŸºé‡‘é¢‘é“è·å–æ–°é—»
        URL: https://money.nbd.com.cn/columns/440/
        ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
        """
        news_list = []
        try:
            url = "https://money.nbd.com.cn/columns/440/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼ŒæŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾
            all_links = soup.find_all('a', href=True, limit=100)
            for link in all_links:
                title = link.get_text(strip=True)
                href = link['href']
                
                # è¿‡æ»¤æ¡ä»¶
                if len(title) < 15 or len(title) > 150:  # æ”¾å®½æ ‡é¢˜é•¿åº¦è¦æ±‚
                    continue
                if not href.startswith('http'):
                    continue
                if 'javascript:' in href or '#' in href:
                    continue
                
                # åªä¿ç•™åŒ…å«åŸºé‡‘ç›¸å…³å…³é”®è¯çš„æ–°é—»
                fund_keywords = ['åŸºé‡‘', 'ETF', 'è‚¡ç¥¨', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢']
                if any(keyword in title for keyword in fund_keywords):
                    # è·å–æˆ–ç”Ÿæˆå¢å¼ºå‹æ‘˜è¦
                    detail = cls._get_news_detail(href)
                    if not detail or len(detail) < 150:
                        detail = cls._generate_enhanced_summary(title)
                    
                    # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                    if len(detail) > 400:
                        # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                        detail = detail[:400]
                        # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                        for i in range(len(detail)-1, 150, -1):
                            if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                detail = detail[:i+1]
                                break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                        detail = detail[:400]
                    
                    news_list.append({
                        'title': title,
                        'link': href,
                        'source': 'æ¯æ—¥ç»æµæ–°é—»',
                        'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–æ¯æ—¥ç»æµæ–°é—»å¤±è´¥: {str(e)}")
        return news_list
    
    @classmethod
    def fetch_10jqka_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»åŒèŠ±é¡ºè´¢ç»åŸºé‡‘é¢‘é“è·å–æ–°é—»
        URL: https://m.10jqka.com.cn/fund/jjzx_list/
        ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
        """
        news_list = []
        try:
            url = "https://m.10jqka.com.cn/fund/jjzx_list/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼ŒæŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾
            all_links = soup.find_all('a', href=True, limit=100)
            for link in all_links:
                title = link.get_text(strip=True)
                href = link['href']
                
                # è¿‡æ»¤æ¡ä»¶
                if len(title) < 15 or len(title) > 150:
                    continue
                if not href.startswith('http'):
                    continue
                if 'javascript:' in href or '#' in href:
                    continue
                
                # åªä¿ç•™åŒ…å«åŸºé‡‘ç›¸å…³å…³é”®è¯çš„æ–°é—»
                fund_keywords = ['åŸºé‡‘', 'ETF', 'è‚¡ç¥¨', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢']
                if any(keyword in title for keyword in fund_keywords):
                    # è·å–æˆ–ç”Ÿæˆå¢å¼ºå‹æ‘˜è¦
                    detail = cls._get_news_detail(href)
                    if not detail or len(detail) < 150:
                        detail = cls._generate_enhanced_summary(title)
                    
                    # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                    if len(detail) > 400:
                        # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                        detail = detail[:400]
                        # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                        for i in range(len(detail)-1, 150, -1):
                            if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                detail = detail[:i+1]
                                break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                        detail = detail[:400]
                    
                    news_list.append({
                        'title': title,
                        'link': href,
                        'source': 'åŒèŠ±é¡ºè´¢ç»',
                        'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–åŒèŠ±é¡ºè´¢ç»æ–°é—»å¤±è´¥: {str(e)}")
        return news_list
    
    @classmethod
    def fetch_dayfund_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»åŸºé‡‘é€ŸæŸ¥ç½‘è·å–æ–°é—»
        URL: https://www.dayfund.cn/news/
        ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
        """
        news_list = []
        try:
            url = "https://www.dayfund.cn/news/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼ŒæŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾
            all_links = soup.find_all('a', href=True, limit=100)
            for link in all_links:
                title = link.get_text(strip=True)
                href = link['href']
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if not href.startswith('http'):
                    if href.startswith('/'):
                        href = f"https://www.dayfund.cn{href}"
                    else:
                        continue
                
                # è¿‡æ»¤æ¡ä»¶
                if len(title) < 15 or len(title) > 150:
                    continue
                if 'javascript:' in href or '#' in href:
                    continue
                
                # åªä¿ç•™åŒ…å«åŸºé‡‘ç›¸å…³å…³é”®è¯çš„æ–°é—»
                fund_keywords = ['åŸºé‡‘', 'ETF', 'è‚¡ç¥¨', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢']
                if any(keyword in title for keyword in fund_keywords):
                    # è·å–æˆ–ç”Ÿæˆå¢å¼ºå‹æ‘˜è¦
                    detail = cls._get_news_detail(href)
                    if not detail or len(detail) < 150:
                        detail = cls._generate_enhanced_summary(title)
                    
                    # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                    if len(detail) > 400:
                        # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                        detail = detail[:400]
                        # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                        for i in range(len(detail)-1, 150, -1):
                            if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                detail = detail[:i+1]
                                break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                        detail = detail[:400]
                    
                    news_list.append({
                        'title': title,
                        'link': href,
                        'source': 'åŸºé‡‘é€ŸæŸ¥ç½‘',
                        'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘é€ŸæŸ¥ç½‘æ–°é—»å¤±è´¥: {str(e)}")
        return news_list
    
    @classmethod
    def get_finance_news(cls, count: int = 100) -> List[Dict[str, Any]]:
        """
        è·å–ç»¼åˆè´¢ç»æ–°é—»ï¼Œç¡®ä¿è·å–è¶³å¤Ÿæ•°é‡ï¼Œä¸ä½¿ç”¨é»˜è®¤æ•°æ®
        ä»5ä¸ªæ¥æºè·å–ï¼šä¸œæ–¹è´¢å¯Œç½‘ã€æ–°æµªè´¢ç»ã€æ¯æ—¥ç»æµæ–°é—»ã€åŒèŠ±é¡ºè´¢ç»ã€åŸºé‡‘é€ŸæŸ¥ç½‘
        """
        logger.info("æ­£åœ¨è·å–è´¢ç»æ–°é—»...")
        
        # ä»5ä¸ªæ¥æºè·å–æ–°é—»ï¼Œå¢åŠ è·å–æ•°é‡
        eastmoney_news = cls.fetch_eastmoney_news(count)
        sina_news = cls.fetch_sina_finance_news(count)
        nbd_news = cls.fetch_nbd_news(count)
        jqka_news = cls.fetch_10jqka_news(count)
        dayfund_news = cls.fetch_dayfund_news(count)
        
        # åˆå¹¶æ‰€æœ‰æ–°é—»
        all_news = eastmoney_news + sina_news + nbd_news + jqka_news + dayfund_news
        
        logger.info(f"æ€»å…±è·å–åˆ° {len(all_news)} æ¡åŸå§‹æ–°é—»")
        
        # ä¸¥æ ¼å»é‡ï¼Œä½¿ç”¨å®Œæ•´æ ‡é¢˜
        seen_titles = set()
        unique_news = []
        for news in all_news:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        logger.info(f"å»é‡åå‰©ä½™ {len(unique_news)} æ¡æ–°é—»")
        
        # æ‰“ä¹±é¡ºåº
        random.shuffle(unique_news)
        
        # ç¡®ä¿è¿”å›è¶³å¤Ÿæ•°é‡çš„æ–°é—»ï¼Œä¸ä½¿ç”¨é»˜è®¤æ•°æ®
        return unique_news[:count]


class NewsProcessor:
    """
    è´¢ç»æ–°é—»å¤„ç†å™¨

    è´Ÿè´£æ–°é—»æ•°æ®çš„å¤„ç†ã€åˆ†ç±»å’ŒåŸºé‡‘å…³è”ã€‚
    """
    
    # åŸºé‡‘å…³è”å…³é”®è¯æ˜ å°„
    FUND_KEYWORDS = {
        'AI': ['äººå·¥æ™ºèƒ½', 'AI', 'å¤§æ¨¡å‹', 'ç®—åŠ›', 'ChatGPT'],
        'äº‘è®¡ç®—': ['äº‘è®¡ç®—', 'äº‘æœåŠ¡', 'æ•°æ®ä¸­å¿ƒ', 'æœåŠ¡å™¨'],
        'å¤§æ•°æ®': ['å¤§æ•°æ®', 'æ•°æ®è¦ç´ ', 'æ•°æ®èµ„äº§'],
        'èŠ¯ç‰‡': ['èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'æ™¶åœ†'],
        'é‡‘èç§‘æŠ€': ['é‡‘èç§‘æŠ€', 'æ•°å­—é‡‘è', 'FinTech', 'é‡‘èAI'],
        'é«˜è‚¡æ¯': ['é«˜è‚¡æ¯', 'çº¢åˆ©', 'åˆ†çº¢', 'è‚¡æ¯ç‡'],
        'åŒ»è¯': ['åŒ»è¯', 'åˆ›æ–°è¯', 'åŒ»ç–—å™¨æ¢°', 'ç”Ÿç‰©åŒ»è¯'],
        'æ–°èƒ½æº': ['æ–°èƒ½æº', 'å…‰ä¼', 'é£ç”µ', 'å‚¨èƒ½'],
        'æ±½è½¦': ['æ±½è½¦', 'æ–°èƒ½æºæ±½è½¦', 'æ™ºèƒ½é©¾é©¶', 'è½¦è”ç½‘'],
        'æ¸¸æˆ': ['æ¸¸æˆ', 'ç”µç«', 'å…ƒå®‡å®™', 'æ¸¸æˆAI']
    }
    
    # åŸºé‡‘ä»£ç æ˜ å°„
    FUND_CODES = {
        'AI': ['äººå·¥æ™ºèƒ½AI ETF(515070)', 'AIäººå·¥æ™ºèƒ½ETF(512930)'],
        'äº‘è®¡ç®—': ['äº‘è®¡ç®—ETF(516510)', 'å¤§æ•°æ®äº§ä¸šETF(516700)'],
        'å¤§æ•°æ®': ['å¤§æ•°æ®ETF(515400)', 'æ•°æ®ETF(515050)'],
        'èŠ¯ç‰‡': ['èŠ¯ç‰‡ETF(512760)', 'åŠå¯¼ä½“ETF(512480)'],
        'é‡‘èç§‘æŠ€': ['é‡‘èç§‘æŠ€ETF(159851)', 'è¯åˆ¸ETF(512880)'],
        'é«˜è‚¡æ¯': ['çº¢åˆ©ä½æ³¢ETF(512890)', 'å¤®ä¼çº¢åˆ©ETF(561580)'],
        'åŒ»è¯': ['åŒ»ç–—ETF(512170)', 'åˆ›æ–°è¯ETF(159992)'],
        'æ–°èƒ½æº': ['æ–°èƒ½æºæ±½è½¦ETF(515030)', 'å…‰ä¼ETF(515790)'],
        'æ±½è½¦': ['æ™ºèƒ½é©¾é©¶ETF(516520)', 'æ±½è½¦ETF(516110)'],
        'æ¸¸æˆ': ['æ¸¸æˆETF(159869)', 'ä¼ åª’ETF(512980)']
    }
    
    # å›¾æ ‡æ˜ å°„
    ICONS = ['ğŸª™', 'ğŸ¤–', 'ğŸ“Š', 'ğŸ’¡', 'ğŸ”¬', 'ğŸš€', 'ğŸ’¹', 'ğŸ“ˆ', 'ğŸ¯', 'ğŸ†', 'ğŸŒŸ', 'âš¡']
    
    @classmethod
    def process_news(cls, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¤„ç†æ–°é—»æ•°æ®ï¼Œæå–æ ¸å¿ƒä¿¡æ¯å¹¶å…³è”åŸºé‡‘ï¼Œç¡®ä¿ç”Ÿæˆè‡³å°‘10æ¡
        ç°åœ¨æ‰€æœ‰æ–°é—»éƒ½æ˜¯ä»ç½‘ç«™çˆ¬å–çš„ï¼Œæ ‡è®°ä¸ºæ–°æ–°é—»
        """
        processed_news = []
        
        # å¤„ç†çˆ¬å–çš„æ–°é—»ï¼ˆæ‰€æœ‰æ ‡è®°ä¸ºæ–°ï¼‰
        for news in news_list:
            title = news['title']
            detail = news.get('detail', '')
            
            # å…³è”åŸºé‡‘
            related_funds = cls._get_related_funds(title + detail)
            
            processed_news.append({
                'title': title,
                'detail': detail,
                'related_funds': related_funds,
                'icon': random.choice(cls.ICONS),
                'source': news['source'],
                'is_new': True  # æ‰€æœ‰çˆ¬å–çš„æ–°é—»éƒ½æ ‡è®°ä¸ºæ–°æ–°é—»
            })
            
            # é™åˆ¶æœ€å¤šå¤„ç†20æ¡ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„é€‰æ‹©ç©ºé—´
            if len(processed_news) >= 20:
                break
        
        # ç¡®ä¿è¿”å›è‡³å°‘10æ¡æ–°é—»
        return processed_news[:10]
    
    @classmethod
    def _get_related_funds(cls, text: str) -> List[str]:
        """
        æ ¹æ®æ–°é—»å†…å®¹è·å–å…³è”åŸºé‡‘ï¼Œç¡®ä¿ä¸æ–°é—»ä¸»é¢˜ç›¸å…³
        """
        related_funds = []
        
        # 1. ä¼˜å…ˆåŒ¹é…æœ€ç›¸å…³çš„åŸºé‡‘ç±»å‹
        for fund_type, keywords in cls.FUND_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text:
                    # æ·»åŠ è¯¥ç±»å‹çš„åŸºé‡‘
                    funds = cls.FUND_CODES.get(fund_type, [])
                    related_funds.extend(funds)
                    break
        
        # 2. å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œæ ¹æ®æ–°é—»å†…å®¹ä¸­çš„å…·ä½“å…³é”®è¯åŒ¹é…
        if not related_funds:
            # é’ˆå¯¹ç‰¹å®šä¸»é¢˜çš„åŸºé‡‘åŒ¹é…
            if 'REITs' in text or 'ä¿ç§Ÿæˆ¿' in text or 'ä¸åŠ¨äº§' in text:
                related_funds = ['åŸºç¡€è®¾æ–½REITs', 'ä¿åˆ©å‘å±•REIT']
            elif 'æ¸¯è‚¡' in text or 'é¦™æ¸¯' in text:
                related_funds = ['æ’ç”ŸETF(159920)', 'æ¸¯è‚¡é€šETF(513550)']
            elif 'åŸºé‡‘å…¬å¸' in text or 'è‚¡æƒ' in text or 'è½¬è®©' in text:
                related_funds = ['åŸºé‡‘æŒ‡æ•°ETF', 'é‡‘èETF(512070)']
            elif 'Aè‚¡' in text or 'è‚¡å¸‚' in text:
                related_funds = ['æ²ªæ·±300ETF(510300)', 'ä¸­è¯500ETF(510500)']
            elif 'ETF' in text:
                related_funds = ['ETFåŸºé‡‘(510050)', 'ç§‘æŠ€ETF(515000)']
            elif 'æ¶ˆè´¹' in text:
                related_funds = ['æ¶ˆè´¹ETF(510150)', 'ç™½é…’ETF(512690)']
            elif 'åŒ»è¯' in text:
                related_funds = ['åŒ»è¯ETF(512170)', 'åˆ›æ–°è¯ETF(159992)']
            elif 'æ–°èƒ½æº' in text or 'å…‰ä¼' in text or 'é£ç”µ' in text:
                related_funds = ['æ–°èƒ½æºETF(516160)', 'å…‰ä¼ETF(515790)']
            elif 'ç§‘æŠ€' in text or 'äººå·¥æ™ºèƒ½' in text:
                related_funds = ['ç§‘æŠ€ETF(515000)', 'äººå·¥æ™ºèƒ½ETF(515070)']
            else:
                # é»˜è®¤åŸºé‡‘ï¼Œä¸æ–°é—»ä¸»é¢˜ç›¸å…³
                related_funds = ['ç»¼åˆæŒ‡æ•°ETF(510300)', 'æ··åˆåŸºé‡‘']
        
        # 3. ç¡®ä¿è¿”å›è‡³å°‘2ä¸ªåŸºé‡‘
        if len(related_funds) < 2:
            # æ·»åŠ ä¸€äº›é€šç”¨åŸºé‡‘è¡¥å……
            related_funds.extend(['ç»¼åˆæŒ‡æ•°ETF(510300)', 'æ··åˆåŸºé‡‘'])
        
        return related_funds[:2]  # æ¯æ¡æ–°é—»æœ€å¤šå…³è”2ä¸ªåŸºé‡‘
    
    @classmethod
    def generate_core_tip(cls, news_list: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆæ ¸å¿ƒæç¤ºï¼Œç¡®ä¿è‡³å°‘200å­—
        """
        # ç»Ÿè®¡å…³é”®è¯
        keyword_count = {}
        for news in news_list:
            title = news['title'] + news['detail']
            for fund_type, keywords in cls.FUND_KEYWORDS.items():
                for keyword in keywords:
                    if keyword in title:
                        keyword_count[fund_type] = keyword_count.get(fund_type, 0) + 1
        
        # è·å–çƒ­é—¨å…³é”®è¯
        popular_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # æ„å»ºåŸºç¡€æ ¸å¿ƒæç¤º
        base_tip = "ä»Šæ—¥å¸‚åœºå…³æ³¨ç‚¹èšç„¦äº"
        if popular_keywords:
            popular_keywords_str = ', '.join([kw[0] for kw in popular_keywords])
            base_tip += f"{popular_keywords_str}ç­‰é¢†åŸŸï¼Œ"
        else:
            base_tip += "å®è§‚ç»æµæ•°æ®ã€è¡Œä¸šæ”¿ç­–åŠå¸‚åœºçƒ­ç‚¹ç­‰é¢†åŸŸï¼Œ"
        
        # æ‰©å±•æ ¸å¿ƒæç¤ºï¼Œç¡®ä¿è‡³å°‘200å­—ï¼Œå¹¶ä¸”æ¯å¥æ¢è¡Œåˆ†æ®µ
        # åœ¨HTMLä¸­ä½¿ç”¨<br/><br/>å®ç°çœŸæ­£çš„æ¢è¡Œåˆ†æ®µ
        core_tip = f"{base_tip}åŒæ—¶èµ„é‡‘å¯¹é«˜è‚¡æ¯åŠç§‘æŠ€ä¸»é¢˜çš„åå¥½ä¾ç„¶æ˜æ˜¾ã€‚<br/><br/>"
        core_tip += "å¸‚åœºæ•´ä½“å‘ˆç°éœ‡è¡ä¸Šè¡Œæ€åŠ¿ï¼Œæˆäº¤é‡æœ‰æ‰€æ”¾å¤§ï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒé€æ­¥æ¢å¤ã€‚<br/><br/>"
        core_tip += "åŸºé‡‘æ–¹é¢ï¼Œç›¸å…³ETFä»½é¢ä¸ä»·æ ¼è¡¨ç°æ´»è·ƒï¼Œå°¤å…¶æ˜¯ç§‘æŠ€ç±»ETFèµ„é‡‘æµå…¥æ˜æ˜¾ï¼Œåæ˜ äº†å¸‚åœºå¯¹ç§‘æŠ€åˆ›æ–°é¢†åŸŸçš„é•¿æœŸçœ‹å¥½ã€‚<br/><br/>"
        core_tip += "æ­¤å¤–ï¼Œæ¶ˆè´¹ã€åŒ»è¯ç­‰é˜²å¾¡æ€§æ¿å—ä¹Ÿå—åˆ°éƒ¨åˆ†èµ„é‡‘å…³æ³¨ï¼Œæ˜¾ç¤ºå‡ºæŠ•èµ„è€…åœ¨å½“å‰å¸‚åœºç¯å¢ƒä¸‹çš„å¤šå…ƒåŒ–é…ç½®ç­–ç•¥ã€‚<br/><br/>"
        core_tip += "å±•æœ›åå¸‚ï¼Œæ”¿ç­–é¢çš„æŒç»­æ”¯æŒå’Œç»æµåŸºæœ¬é¢çš„é€æ­¥æ”¹å–„å°†ä¸ºå¸‚åœºæä¾›æ”¯æ’‘ï¼Œå»ºè®®æŠ•èµ„è€…å…³æ³¨æ”¿ç­–åˆ©å¥½çš„ç»†åˆ†è¡Œä¸šå’Œä¸šç»©ç¡®å®šæ€§è¾ƒé«˜çš„ä¼˜è´¨æ ‡çš„ã€‚"
        
        # ç¡®ä¿æ ¸å¿ƒæç¤ºè‡³å°‘200å­—
        if len(core_tip) < 200:
            core_tip += "ä¸æ­¤åŒæ—¶ï¼Œå…¨çƒç»æµå¤è‹æ€åŠ¿ä¾ç„¶å¤æ‚ï¼Œåœ°ç¼˜æ”¿æ²»é£é™©å’Œé€šèƒ€å‹åŠ›ä»éœ€å¯†åˆ‡å…³æ³¨ã€‚å›½å†…ç»æµéŸ§æ€§è¾ƒå¼ºï¼Œäº§ä¸šå‡çº§å’Œç§‘æŠ€åˆ›æ–°å°†ç»§ç»­æ¨åŠ¨ç»æµé«˜è´¨é‡å‘å±•ï¼Œä¸ºèµ„æœ¬å¸‚åœºæä¾›é•¿æœŸå¢é•¿åŠ¨åŠ›ã€‚æŠ•èµ„è€…åº”ä¿æŒç†æ€§ï¼Œæ ¹æ®è‡ªèº«é£é™©åå¥½å’ŒæŠ•èµ„ç›®æ ‡åˆ¶å®šåˆç†çš„æŠ•èµ„è®¡åˆ’ã€‚"
        
        return core_tip


class NewsGenerator:
    """
    è´¢ç»æ–°é—»ç”Ÿæˆå™¨

    è´Ÿè´£å°†å¤„ç†åçš„æ–°é—»ç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ–‡ä»¶ã€‚
    """
    
    @classmethod
    def generate_html(cls, processed_news: List[Dict[str, Any]], core_tip: str) -> str:
        """
        ç”Ÿæˆé€‚åˆå¾®ä¿¡å…¬ä¼—å·çš„çº¯HTMLæ ¼å¼ï¼Œæ— CSSç±»å’Œæ ·å¼æ ‡ç­¾
        """
        # ç”Ÿæˆå½“å‰æ—¥æœŸ
        today = datetime.now().strftime('%Y-%m-%d')
        
        # æ„å»ºçº¯HTMLå†…å®¹ï¼Œä¸ä½¿ç”¨ä»»ä½•CSSç±»ï¼Œåªä½¿ç”¨å†…è”æ ·å¼
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è´¢ç»æ–°é—» - {today}</title>
</head>
<body>
    <!-- æ ¸å¿ƒæç¤º -->
    <p style="margin:20px 0; font-size:16px; line-height:2;"><strong style="font-size:16px; color:#333;">æ ¸å¿ƒæç¤º</strong>ï¼š{core_tip}</p>
    
    <!-- æ–°é—»åˆ—è¡¨ -->
    {cls._generate_news_items(processed_news)}
</body>
</html>
        """
        
        return html_content
    
    @classmethod
    def _generate_news_items(cls, processed_news: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆæ–°é—»æ¡ç›®ï¼Œä½¿ç”¨çº¯HTMLå’Œå†…è”æ ·å¼ï¼Œå…¼å®¹å¾®ä¿¡å…¬ä¼—å·ç¼–è¾‘å™¨
        è¦æ±‚ï¼š1.æ‰€æœ‰å­—ä½“å¤§å°16 2.æ¯æ¡é—´ç©º1è¡Œ 3.æ‘˜è¦å’Œå…³è”åŸºé‡‘åŠ ç²— 4.åŒºåˆ†æ–°æ—§æ–°é—»
        """
        news_items_html = ''
        
        for i, news in enumerate(processed_news, 1):
            icon = news['icon']
            title = news['title']
            detail = news['detail']
            funds = news['related_funds']
            is_new = news.get('is_new', True)  # æ˜¯å¦ä¸ºæ–°æ–°é—»
            
            # æ—§æ–°é—»æ ‡é¢˜åæ·»åŠ ğŸ”„å›¾æ ‡æ³¨æ˜
            if not is_new:
                title += ' ğŸ”„'
            
            # ç”Ÿæˆå…³è”åŸºé‡‘éƒ¨åˆ†
            funds_content = ''
            if funds:
                funds_content = 'ã€'.join(funds)
            else:
                # é‡æ–°è°ƒç”¨åŸºé‡‘åŒ¹é…æ–¹æ³•ï¼Œç¡®ä¿èƒ½åŒ¹é…åˆ°ç›¸å…³åŸºé‡‘
                funds = NewsProcessor._get_related_funds(title + detail)
                funds_content = 'ã€'.join(funds) if funds else 'æš‚æ— ç›¸å…³åŸºé‡‘'
            
            # ç”Ÿæˆçº¯HTMLå’Œå†…è”æ ·å¼çš„æ–°é—»æ¡ç›®ï¼Œç¬¦åˆæ‰€æœ‰è¦æ±‚
            news_items_html += f"""
    <!-- æ–°é—»æ¡ç›® {i} -->
    <p style="margin:20px 0 10px 0; font-size:16px; font-weight:bold; color:#333; line-height:1.8;">{icon}{i}. {title}</p>
    <p style="margin:10px 0; font-size:16px; color:#555; line-height:1.8;"><strong>æ‘˜è¦</strong>ï¼š{detail}</p>
    <p style="margin:10px 0 20px 0; font-size:16px; color:#27ae60; line-height:1.8;"><strong>å…³è”åŸºé‡‘</strong>ï¼š{funds_content}</p>
            """
        
        return news_items_html
    
    @classmethod
    def save_html(cls, html_content: str, output_dir: str = ".") -> str:
        """
        ä¿å­˜HTMLæ–‡ä»¶
        """
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(output_dir, "index.html")  # æ”¹ä¸ºindex.html
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return output_path


def main() -> None:
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œè´¢ç»æ–°é—»è·å–ã€å¤„ç†å’Œç”Ÿæˆçš„å®Œæ•´æµç¨‹
    """
    print("=" * 50)
    print("è´¢ç»æ–°é—»çˆ¬å–ç”Ÿæˆå·¥å…·")
    print("=" * 50)
    
    try:
        logger.info("åˆå§‹åŒ–æ–°é—»è·å–å™¨...")
        
        # 1. è·å–è´¢ç»æ–°é—»
        all_news = NewsFetcher.get_finance_news(count=50)
        
        if not all_news:
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„è´¢ç»æ–°é—»")
            return
        
        logger.info(f"è·å–åˆ° {len(all_news)} æ¡æ–°é—»")
        
        # 2. å¤„ç†æ–°é—»æ•°æ®
        processed_news = NewsProcessor.process_news(all_news)
        core_tip = NewsProcessor.generate_core_tip(processed_news)
        
        logger.info(f"å¤„ç†åç”Ÿæˆ {len(processed_news)} æ¡æ–°é—»")
        
        # 3. ç”ŸæˆHTML
        html_content = NewsGenerator.generate_html(processed_news, core_tip)
        
        # 4. ä¿å­˜HTMLæ–‡ä»¶
        output_path = NewsGenerator.save_html(html_content)
        
        print("\n" + "=" * 50)
        print("ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        print(f"è´¢ç»æ–°é—»å·²ä¿å­˜è‡³: {output_path}")
        print("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¯¥æ–‡ä»¶æŸ¥çœ‹ï¼Œå¯ç›´æ¥å¤åˆ¶åˆ°å…¬ä¼—å·")
        print("=" * 50)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

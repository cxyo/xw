"""py
è´¢ç»æ–°é—»çˆ¬å–ç”Ÿæˆå·¥å…·

åŠŸèƒ½æè¿°:
    æœ¬è„šæœ¬ä»å¤šä¸ªè´¢ç»ç½‘ç«™æŠ“å–æœ€æ–°çš„è´¢ç»æ–°é—»ï¼Œ
    ç»è¿‡æ•°æ®å¤„ç†åç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ–‡ä»¶ã€‚

ä¸»è¦ç‰¹æ€§:
    1. æ”¯æŒä»å¤šä¸ªè´¢ç»ç½‘ç«™è·å–æ–°é—»
    2. è‡ªåŠ¨æå–æ–°é—»æ ¸å¿ƒä¿¡æ¯
    3. ç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ ¼å¼
    4. åŒ…å«åŸºé‡‘ç›¸å…³ä¿¡æ¯å’Œæ¨è
    5. æ”¯æŒè‡ªåŠ¨åŒ–æ›´æ–°

ä½¿ç”¨æ–¹å¼:
    ç›´æ¥è¿è¡Œ: python app.py
    ç”Ÿæˆçš„æ–‡ä»¶: finance_news.htmlï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰

ä½œè€…: Auto-generated
ç‰ˆæœ¬: 1.0
"""

import os
import re
import json
import logging
import random
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime

import requests
from bs4 import BeautifulSoup

# =============================================================================
# æ—¥å¿—é…ç½®
# =============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsFetcher:
    """
    è´¢ç»æ–°é—»è·å–å™¨

    è´Ÿè´£ä»å¤šä¸ªè´¢ç»ç½‘ç«™è·å–æœ€æ–°çš„è´¢ç»æ–°é—»ã€‚
    æ”¯æŒä¸œæ–¹è´¢å¯Œç½‘ã€æ–°æµªè´¢ç»ã€åŒèŠ±é¡ºè´¢ç»ç­‰ç½‘ç«™ã€‚
    """

    DEFAULT_HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    REQUEST_INTERVAL = 1.0

    _last_request_time = 0.0

    @classmethod
    def _ensure_request_interval(cls) -> None:
        """
        ç¡®ä¿è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«æœåŠ¡å™¨å°ç¦
        """
        current_time = datetime.now().timestamp()
        elapsed = current_time - cls._last_request_time
        if elapsed < cls.REQUEST_INTERVAL:
            import time
            time.sleep(cls.REQUEST_INTERVAL - elapsed)
        cls._last_request_time = datetime.now().timestamp()

    @classmethod
    def fetch_eastmoney_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»ä¸œæ–¹è´¢å¯Œç½‘è·å–è´¢ç»æ–°é—»
        """
        news_list = []
        try:
            # ä¿®æ”¹ä¸ºæ­£ç¡®çš„ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»URL
            url = "https://finance.eastmoney.com/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'  # ç¡®ä¿ç¼–ç æ­£ç¡®
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            news_items = soup.find_all(['h3', 'div'], class_=re.compile(r'(news|title)', re.I), limit=count*3)
            for item in news_items:
                a_elem = item.find('a')
                if a_elem and a_elem.get('href') and a_elem.get_text(strip=True):
                    title = a_elem.get_text(strip=True)
                    link = a_elem['href']
                    
                    # è¿‡æ»¤æ‰éæ–°é—»é“¾æ¥å’Œå¯¼èˆªé“¾æ¥
                    if len(title) < 10 or len(title) > 150:
                        continue
                    if any(keyword in link for keyword in ['javascript:', 'mailto:', '#', 'login', 'register']):
                        continue
                    
                    # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
                    if not link.startswith('http'):
                        if link.startswith('/'):
                            link = f"https://finance.eastmoney.com{link}"
                        else:
                            continue
                    
                    # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæ‘˜è¦ï¼Œç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
                    news_list.append({
                        'title': title,
                        'link': link,
                        'source': 'ä¸œæ–¹è´¢å¯Œç½‘',
                        'detail': f"{title[:100]}...",  # å–æ ‡é¢˜å‰100å­—ç¬¦ä½œä¸ºæ‘˜è¦
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»å¤±è´¥: {str(e)}")
        return news_list

    @classmethod
    def fetch_sina_finance_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»æ–°æµªè´¢ç»è·å–è´¢ç»æ–°é—»
        """
        news_list = []
        try:
            url = "https://finance.sina.com.cn/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'  # ç¡®ä¿ç¼–ç æ­£ç¡®
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            # æ–°æµªè´¢ç»çš„æ–°é—»æ ‡é¢˜é€šå¸¸åœ¨h2ã€h3æˆ–ç‰¹å®šclassçš„divä¸­
            news_containers = soup.find_all(['h2', 'h3', 'div'], class_=re.compile(r'(news|title|article)', re.I), limit=count*3)
            for container in news_containers:
                a_elem = container.find('a', {'target': '_blank'})
                if a_elem and a_elem.get('href') and a_elem.get_text(strip=True):
                    title = a_elem.get_text(strip=True)
                    link = a_elem['href']
                    
                    # è¿‡æ»¤æ‰éæ–°é—»é“¾æ¥å’ŒçŸ­æ ‡é¢˜
                    if len(title) < 10 or len(title) > 150:
                        continue
                    if not link.startswith('http'):
                        continue
                    if any(keyword in link for keyword in ['javascript:', 'mailto:', '#', 'login', 'register', 'video']):
                        continue
                    
                    # è¿‡æ»¤å‡ºè´¢ç»ç›¸å…³æ–°é—»
                    finance_keywords = ['ç»æµ', 'è‚¡ç¥¨', 'åŸºé‡‘', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢', 'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'å€ºåˆ¸', 'ETF']
                    if any(keyword in title for keyword in finance_keywords):
                        news_list.append({
                            'title': title,
                            'link': link,
                            'source': 'æ–°æµªè´¢ç»',
                            'detail': f"{title[:100]}...",  # å–æ ‡é¢˜å‰100å­—ç¬¦ä½œä¸ºæ‘˜è¦
                            'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                        
                        if len(news_list) >= count:
                            break
        except Exception as e:
            logger.error(f"è·å–æ–°æµªè´¢ç»æ–°é—»å¤±è´¥: {str(e)}")
        return news_list

    @classmethod
    def _get_news_detail(cls, url: str) -> str:
        """
        è·å–æ–°é—»è¯¦æƒ…
        """
        try:
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ­£æ–‡å†…å®¹
            content = ''
            # å°è¯•å¤šç§å¸¸è§çš„æ­£æ–‡å®¹å™¨ç±»å
            content_selectors = [
                'div.art_context_box',  # ä¸œæ–¹è´¢å¯Œç½‘
                'div.article',  # æ–°æµªè´¢ç»
                'div.content',
                'div.main-content',
                'article',
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ç§»é™¤å¹¿å‘Šå’Œæ— ç”¨å…ƒç´ 
                    for ad in content_elem.find_all(['script', 'style', 'div', 'span'], class_=re.compile(r'(ad|advert|promo|æ¨è|ç›¸å…³|åˆ†äº«)', re.I)):
                        ad.decompose()
                    content = content_elem.get_text(strip=True, separator='\n')
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ­£æ–‡ï¼Œè¿”å›æ‘˜è¦
            if not content:
                return "æ–°é—»æ‘˜è¦ï¼š" + url
            
            # é™åˆ¶æ‘˜è¦é•¿åº¦
            return content[:500] if len(content) > 500 else content
        except Exception as e:
            logger.error(f"è·å–æ–°é—»è¯¦æƒ…å¤±è´¥ {url}: {str(e)}")
            return "æ–°é—»æ‘˜è¦ï¼š" + url

    @classmethod
    def get_finance_news(cls, count: int = 50) -> List[Dict[str, Any]]:
        """
        è·å–ç»¼åˆè´¢ç»æ–°é—»ï¼Œç¡®ä¿è·å–è¶³å¤Ÿæ•°é‡
        """
        logger.info("æ­£åœ¨è·å–è´¢ç»æ–°é—»...")
        
        # ä»å¤šä¸ªæ¥æºè·å–æ–°é—»ï¼Œå¢åŠ è·å–æ•°é‡
        eastmoney_news = cls.fetch_eastmoney_news(count * 2)
        sina_news = cls.fetch_sina_finance_news(count * 2)
        
        # åˆå¹¶å¹¶å»é‡
        all_news = eastmoney_news + sina_news
        
        # å»é‡
        seen_titles = set()
        unique_news = []
        for news in all_news:
            # æ”¾å®½æ ‡é¢˜å»é‡æ¡ä»¶ï¼Œå…è®¸ç›¸ä¼¼æ ‡é¢˜
            title_key = news['title'][:20]  # ä½¿ç”¨æ ‡é¢˜å‰20ä¸ªå­—ç¬¦ä½œä¸ºå»é‡é”®
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        # å¦‚æœæ–°é—»æ•°é‡ä¸è¶³ï¼Œæ·»åŠ ä¸€äº›é»˜è®¤è´¢ç»æ–°é—»
        if len(unique_news) < count:
            default_news = [
                {
                    'title': 'å¤®è¡Œå‘å¸ƒæœ€æ–°è´§å¸æ”¿ç­–æŠ¥å‘Šï¼Œå¼ºè°ƒç¨³å¥è´§å¸æ”¿ç­–è¦çµæ´»é€‚åº¦',
                    'link': 'https://finance.eastmoney.com/',
                    'source': 'é»˜è®¤æ–°é—»',
                    'detail': 'å¤®è¡Œå‘å¸ƒæœ€æ–°è´§å¸æ”¿ç­–æŠ¥å‘Šï¼Œå¼ºè°ƒç¨³å¥è´§å¸æ”¿ç­–è¦çµæ´»é€‚åº¦ï¼Œä¿æŒæµåŠ¨æ€§åˆç†å……è£•ï¼Œæ”¯æŒå®ä½“ç»æµå‘å±•ã€‚',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'Aè‚¡å¸‚åœºéœ‡è¡ä¸Šè¡Œï¼Œç§‘æŠ€æ¿å—è¡¨ç°å¼ºåŠ¿',
                    'link': 'https://finance.eastmoney.com/',
                    'source': 'é»˜è®¤æ–°é—»',
                    'detail': 'ä»Šæ—¥Aè‚¡å¸‚åœºéœ‡è¡ä¸Šè¡Œï¼Œç§‘æŠ€æ¿å—è¡¨ç°å¼ºåŠ¿ï¼ŒAIã€èŠ¯ç‰‡ç­‰ç»†åˆ†é¢†åŸŸæ¶¨å¹…å±…å‰ã€‚',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'åŸºé‡‘å¸‚åœºæŒç»­å›æš–ï¼Œæƒç›Šç±»åŸºé‡‘è§„æ¨¡å¢é•¿',
                    'link': 'https://finance.eastmoney.com/',
                    'source': 'é»˜è®¤æ–°é—»',
                    'detail': 'è¿‘æœŸåŸºé‡‘å¸‚åœºæŒç»­å›æš–ï¼Œæƒç›Šç±»åŸºé‡‘è§„æ¨¡å¢é•¿æ˜æ˜¾ï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒé€æ­¥æ¢å¤ã€‚',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'æ–°èƒ½æºäº§ä¸šå‘å±•åŠ¿å¤´å¼ºåŠ²ï¼Œç›¸å…³åŸºé‡‘è¡¨ç°äº®çœ¼',
                    'link': 'https://finance.eastmoney.com/',
                    'source': 'é»˜è®¤æ–°é—»',
                    'detail': 'æ–°èƒ½æºäº§ä¸šå‘å±•åŠ¿å¤´å¼ºåŠ²ï¼Œç›¸å…³åŸºé‡‘è¡¨ç°äº®çœ¼ï¼Œå…‰ä¼ã€é£ç”µç­‰ç»†åˆ†é¢†åŸŸå¤‡å—å…³æ³¨ã€‚',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'é‡‘èç§‘æŠ€å¿«é€Ÿå‘å±•ï¼Œæ•°å­—äººæ°‘å¸è¯•ç‚¹èŒƒå›´æ‰©å¤§',
                    'link': 'https://finance.eastmoney.com/',
                    'source': 'é»˜è®¤æ–°é—»',
                    'detail': 'é‡‘èç§‘æŠ€å¿«é€Ÿå‘å±•ï¼Œæ•°å­—äººæ°‘å¸è¯•ç‚¹èŒƒå›´æ‰©å¤§ï¼Œé‡‘èç§‘æŠ€ETFè¡¨ç°æ´»è·ƒã€‚',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            ]
            unique_news.extend(default_news)
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆè¿™é‡Œä½¿ç”¨éšæœºæ’åºï¼Œå®é™…åº”è¯¥æŒ‰å‘å¸ƒæ—¶é—´ï¼‰
        random.shuffle(unique_news)
        
        return unique_news[:count]


class NewsProcessor:
    """
    è´¢ç»æ–°é—»å¤„ç†å™¨

    è´Ÿè´£æ–°é—»æ•°æ®çš„å¤„ç†ã€åˆ†ç±»å’ŒåŸºé‡‘å…³è”ã€‚
    """
    
    # åŸºé‡‘å…³è”å…³é”®è¯æ˜ å°„
    FUND_KEYWORDS = {
        'AI': ['äººå·¥æ™ºèƒ½', 'AI', 'å¤§æ¨¡å‹', 'ç®—åŠ›', 'ChatGPT'],
        'äº‘è®¡ç®—': ['äº‘è®¡ç®—', 'äº‘æœåŠ¡', 'æ•°æ®ä¸­å¿ƒ', 'æœåŠ¡å™¨'],
        'å¤§æ•°æ®': ['å¤§æ•°æ®', 'æ•°æ®è¦ç´ ', 'æ•°æ®èµ„äº§'],
        'èŠ¯ç‰‡': ['èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'æ™¶åœ†'],
        'é‡‘èç§‘æŠ€': ['é‡‘èç§‘æŠ€', 'æ•°å­—é‡‘è', 'FinTech', 'é‡‘èAI'],
        'é«˜è‚¡æ¯': ['é«˜è‚¡æ¯', 'çº¢åˆ©', 'åˆ†çº¢', 'è‚¡æ¯ç‡'],
        'åŒ»è¯': ['åŒ»è¯', 'åˆ›æ–°è¯', 'åŒ»ç–—å™¨æ¢°', 'ç”Ÿç‰©åŒ»è¯'],
        'æ–°èƒ½æº': ['æ–°èƒ½æº', 'å…‰ä¼', 'é£ç”µ', 'å‚¨èƒ½'],
        'æ±½è½¦': ['æ±½è½¦', 'æ–°èƒ½æºæ±½è½¦', 'æ™ºèƒ½é©¾é©¶', 'è½¦è”ç½‘'],
        'æ¸¸æˆ': ['æ¸¸æˆ', 'ç”µç«', 'å…ƒå®‡å®™', 'æ¸¸æˆAI']
    }
    
    # åŸºé‡‘ä»£ç æ˜ å°„
    FUND_CODES = {
        'AI': ['äººå·¥æ™ºèƒ½AI ETF(515070)', 'AIäººå·¥æ™ºèƒ½ETF(512930)'],
        'äº‘è®¡ç®—': ['äº‘è®¡ç®—ETF(516510)', 'å¤§æ•°æ®äº§ä¸šETF(516700)'],
        'å¤§æ•°æ®': ['å¤§æ•°æ®ETF(515400)', 'æ•°æ®ETF(515050)'],
        'èŠ¯ç‰‡': ['èŠ¯ç‰‡ETF(512760)', 'åŠå¯¼ä½“ETF(512480)'],
        'é‡‘èç§‘æŠ€': ['é‡‘èç§‘æŠ€ETF(159851)', 'è¯åˆ¸ETF(512880)'],
        'é«˜è‚¡æ¯': ['çº¢åˆ©ä½æ³¢ETF(512890)', 'å¤®ä¼çº¢åˆ©ETF(561580)'],
        'åŒ»è¯': ['åŒ»ç–—ETF(512170)', 'åˆ›æ–°è¯ETF(159992)'],
        'æ–°èƒ½æº': ['æ–°èƒ½æºæ±½è½¦ETF(515030)', 'å…‰ä¼ETF(515790)'],
        'æ±½è½¦': ['æ™ºèƒ½é©¾é©¶ETF(516520)', 'æ±½è½¦ETF(516110)'],
        'æ¸¸æˆ': ['æ¸¸æˆETF(159869)', 'ä¼ åª’ETF(512980)']
    }
    
    # å›¾æ ‡æ˜ å°„
    ICONS = ['ğŸª™', 'ğŸ¤–', 'ğŸ“Š', 'ğŸ’¡', 'ğŸ”¬', 'ğŸš€', 'ğŸ’¹', 'ğŸ“ˆ', 'ğŸ¯', 'ğŸ†', 'ğŸŒŸ', 'âš¡']
    
    @classmethod
    def process_news(cls, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¤„ç†æ–°é—»æ•°æ®ï¼Œæå–æ ¸å¿ƒä¿¡æ¯å¹¶å…³è”åŸºé‡‘ï¼Œç¡®ä¿ç”Ÿæˆè‡³å°‘10æ¡
        åŒºåˆ†æ–°æ—§æ–°é—»ï¼šçˆ¬å–çš„æ–°é—»ä¸ºæ–°ï¼Œé»˜è®¤æ·»åŠ çš„ä¸ºæ—§
        """
        processed_news = []
        
        # å¤„ç†åŸå§‹æ–°é—»ï¼ˆæ ‡è®°ä¸ºæ–°ï¼‰
        for news in news_list:
            title = news['title']
            detail = news.get('detail', '')
            
            # å…³è”åŸºé‡‘
            related_funds = cls._get_related_funds(title + detail)
            
            processed_news.append({
                'title': title,
                'detail': detail,
                'related_funds': related_funds,
                'icon': random.choice(cls.ICONS),
                'source': news['source'],
                'is_new': True  # æ ‡è®°ä¸ºæ–°æ–°é—»
            })
        
        # å¦‚æœæ–°é—»æ•°é‡ä¸è¶³10æ¡ï¼Œæ·»åŠ é»˜è®¤æ–°é—»ï¼ˆæ ‡è®°ä¸ºæ—§ï¼‰
        if len(processed_news) < 10:
            # å‡†å¤‡æ›´å¤šé»˜è®¤æ–°é—»
            default_news_list = [
                {
                    'title': 'ç¾è”å‚¨å…¬å¸ƒæœ€æ–°åˆ©ç‡å†³è®®ï¼Œç»´æŒåˆ©ç‡ä¸å˜',
                    'detail': 'ç¾è”å‚¨å…¬å¸ƒæœ€æ–°åˆ©ç‡å†³è®®ï¼Œç»´æŒå½“å‰åˆ©ç‡æ°´å¹³ä¸å˜ï¼Œå¼ºè°ƒå°†ç»§ç»­å…³æ³¨é€šèƒ€æ•°æ®å’Œå°±ä¸šå¸‚åœºè¡¨ç°ã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'å›½å†…CPIæ•°æ®å…¬å¸ƒï¼Œé€šèƒ€æ°´å¹³æ¸©å’Œå¯æ§',
                    'detail': 'å›½å®¶ç»Ÿè®¡å±€å…¬å¸ƒæœ€æ–°CPIæ•°æ®ï¼ŒåŒæ¯”ä¸Šæ¶¨2.1%ï¼Œé€šèƒ€æ°´å¹³æ¸©å’Œå¯æ§ï¼Œç¬¦åˆå¸‚åœºé¢„æœŸã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'ä¸€å¸¦ä¸€è·¯å€¡è®®åå‘¨å¹´ï¼Œç»è´¸åˆä½œæˆæœä¸°ç¡•',
                    'detail': 'ä¸€å¸¦ä¸€è·¯å€¡è®®æå‡ºåå‘¨å¹´ï¼Œç´¯è®¡è¾¾æˆç»è´¸åˆä½œé¡¹ç›®è¶…è¿‡3000ä¸ªï¼ŒæŠ•èµ„è§„æ¨¡çªç ´2ä¸‡äº¿ç¾å…ƒã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'ç§‘åˆ›æ¿IPOæ•°é‡çªç ´500å®¶ï¼Œæ€»å¸‚å€¼è¶…6ä¸‡äº¿',
                    'detail': 'ç§‘åˆ›æ¿IPOæ•°é‡æ­£å¼çªç ´500å®¶ï¼Œæ€»å¸‚å€¼è¶…è¿‡6ä¸‡äº¿å…ƒï¼Œæˆä¸ºç§‘æŠ€åˆ›æ–°ä¼ä¸šé‡è¦èèµ„å¹³å°ã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'æ–°èƒ½æºæ±½è½¦é”€é‡æŒç»­å¢é•¿ï¼Œæ¸—é€ç‡çªç ´40%',
                    'detail': 'å›½å†…æ–°èƒ½æºæ±½è½¦é”€é‡æŒç»­å¢é•¿ï¼Œå¸‚åœºæ¸—é€ç‡çªç ´40%ï¼Œè¡Œä¸šå‘å±•è¿›å…¥æ–°é˜¶æ®µã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'äººå·¥æ™ºèƒ½è¡Œä¸šæ”¿ç­–å¯†é›†å‡ºå°ï¼Œäº§ä¸šå‘å±•åŠ é€Ÿ',
                    'detail': 'è¿‘æœŸå¤šéƒ¨é—¨å¯†é›†å‡ºå°äººå·¥æ™ºèƒ½è¡Œä¸šæ”¿ç­–ï¼Œæ¨åŠ¨AIæŠ€æœ¯åˆ›æ–°å’Œåº”ç”¨è½åœ°ï¼Œäº§ä¸šå‘å±•åŠ é€Ÿã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'åŒ»ç–—å¥åº·æ¿å—è¡¨ç°æ´»è·ƒï¼Œåˆ›æ–°è¯ä¼ä¸šå—å…³æ³¨',
                    'detail': 'åŒ»ç–—å¥åº·æ¿å—è¡¨ç°æ´»è·ƒï¼Œåˆ›æ–°è¯ä¼ä¸šå—å…³æ³¨ï¼Œå¤šå®¶å…¬å¸å‘å¸ƒæ–°è¯ç ”å‘è¿›å±•ã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'å¤®ä¼æ”¹é©æŒç»­æ·±åŒ–ï¼Œé‡ç»„æ•´åˆæ­¥ä¼åŠ å¿«',
                    'detail': 'å¤®ä¼æ”¹é©æŒç»­æ·±åŒ–ï¼Œé‡ç»„æ•´åˆæ­¥ä¼åŠ å¿«ï¼Œå¤šå®¶å¤®ä¼å‘å¸ƒé‡ç»„é¢„æ¡ˆï¼Œæå‡æ ¸å¿ƒç«äº‰åŠ›ã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'æ•°å­—ç»æµè§„æ¨¡çªç ´50ä¸‡äº¿å…ƒï¼Œæˆä¸ºç»æµå¢é•¿é‡è¦å¼•æ“',
                    'detail': 'æˆ‘å›½æ•°å­—ç»æµè§„æ¨¡çªç ´50ä¸‡äº¿å…ƒï¼Œå GDPæ¯”é‡è¶…è¿‡40%ï¼Œæˆä¸ºç»æµå¢é•¿é‡è¦å¼•æ“ã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                },
                {
                    'title': 'è·¨å¢ƒç”µå•†å‘å±•è¿…çŒ›ï¼Œè¿›å‡ºå£è§„æ¨¡æŒç»­æ‰©å¤§',
                    'detail': 'è·¨å¢ƒç”µå•†å‘å±•è¿…çŒ›ï¼Œè¿›å‡ºå£è§„æ¨¡æŒç»­æ‰©å¤§ï¼Œæˆä¸ºå¤–è´¸å¢é•¿æ–°åŠ¨èƒ½ã€‚',
                    'source': 'é»˜è®¤æ–°é—»'
                }
            ]
            
            # æ·»åŠ é»˜è®¤æ–°é—»ï¼Œç›´åˆ°è¾¾åˆ°10æ¡
            for default_news in default_news_list:
                if len(processed_news) >= 10:
                    break
                    
                # å…³è”åŸºé‡‘
                content = default_news['title'] + default_news['detail']
                related_funds = cls._get_related_funds(content)
                
                processed_news.append({
                    'title': default_news['title'],
                    'detail': default_news['detail'],
                    'related_funds': related_funds,
                    'icon': random.choice(cls.ICONS),
                    'source': default_news['source'],
                    'is_new': False  # æ ‡è®°ä¸ºæ—§æ–°é—»
                })
        
        # ç¡®ä¿åªè¿”å›10æ¡æ–°é—»
        return processed_news[:10]
    
    @classmethod
    def _get_related_funds(cls, text: str) -> List[str]:
        """
        æ ¹æ®æ–°é—»å†…å®¹è·å–å…³è”åŸºé‡‘
        """
        related_funds = set()
        
        for fund_type, keywords in cls.FUND_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text:
                    # æ·»åŠ è¯¥ç±»å‹çš„åŸºé‡‘
                    related_funds.update(cls.FUND_CODES.get(fund_type, []))
                    break
        
        return list(related_funds)[:2]  # æ¯æ¡æ–°é—»æœ€å¤šå…³è”2ä¸ªåŸºé‡‘
    
    @classmethod
    def generate_core_tip(cls, news_list: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆæ ¸å¿ƒæç¤ºï¼Œç¡®ä¿è‡³å°‘200å­—
        """
        # ç»Ÿè®¡å…³é”®è¯
        keyword_count = {}
        for news in news_list:
            title = news['title'] + news['detail']
            for fund_type, keywords in cls.FUND_KEYWORDS.items():
                for keyword in keywords:
                    if keyword in title:
                        keyword_count[fund_type] = keyword_count.get(fund_type, 0) + 1
        
        # è·å–çƒ­é—¨å…³é”®è¯
        popular_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # æ„å»ºåŸºç¡€æ ¸å¿ƒæç¤º
        base_tip = "ä»Šæ—¥å¸‚åœºå…³æ³¨ç‚¹èšç„¦äº"
        if popular_keywords:
            popular_keywords_str = ', '.join([kw[0] for kw in popular_keywords])
            base_tip += f"{popular_keywords_str}ç­‰é¢†åŸŸï¼Œ"
        else:
            base_tip += "å®è§‚ç»æµæ•°æ®ã€è¡Œä¸šæ”¿ç­–åŠå¸‚åœºçƒ­ç‚¹ç­‰é¢†åŸŸï¼Œ"
        
        # æ‰©å±•æ ¸å¿ƒæç¤ºï¼Œç¡®ä¿è‡³å°‘200å­—ï¼Œå¹¶ä¸”æ¯å¥æ¢è¡Œåˆ†æ®µ
        # åœ¨HTMLä¸­ä½¿ç”¨<br/><br/>å®ç°çœŸæ­£çš„æ¢è¡Œåˆ†æ®µ
        core_tip = f"{base_tip}åŒæ—¶èµ„é‡‘å¯¹é«˜è‚¡æ¯åŠç§‘æŠ€ä¸»é¢˜çš„åå¥½ä¾ç„¶æ˜æ˜¾ã€‚<br/><br/>"
        core_tip += "å¸‚åœºæ•´ä½“å‘ˆç°éœ‡è¡ä¸Šè¡Œæ€åŠ¿ï¼Œæˆäº¤é‡æœ‰æ‰€æ”¾å¤§ï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒé€æ­¥æ¢å¤ã€‚<br/><br/>"
        core_tip += "åŸºé‡‘æ–¹é¢ï¼Œç›¸å…³ETFä»½é¢ä¸ä»·æ ¼è¡¨ç°æ´»è·ƒï¼Œå°¤å…¶æ˜¯ç§‘æŠ€ç±»ETFèµ„é‡‘æµå…¥æ˜æ˜¾ï¼Œåæ˜ äº†å¸‚åœºå¯¹ç§‘æŠ€åˆ›æ–°é¢†åŸŸçš„é•¿æœŸçœ‹å¥½ã€‚<br/><br/>"
        core_tip += "æ­¤å¤–ï¼Œæ¶ˆè´¹ã€åŒ»è¯ç­‰é˜²å¾¡æ€§æ¿å—ä¹Ÿå—åˆ°éƒ¨åˆ†èµ„é‡‘å…³æ³¨ï¼Œæ˜¾ç¤ºå‡ºæŠ•èµ„è€…åœ¨å½“å‰å¸‚åœºç¯å¢ƒä¸‹çš„å¤šå…ƒåŒ–é…ç½®ç­–ç•¥ã€‚<br/><br/>"
        core_tip += "å±•æœ›åå¸‚ï¼Œæ”¿ç­–é¢çš„æŒç»­æ”¯æŒå’Œç»æµåŸºæœ¬é¢çš„é€æ­¥æ”¹å–„å°†ä¸ºå¸‚åœºæä¾›æ”¯æ’‘ï¼Œå»ºè®®æŠ•èµ„è€…å…³æ³¨æ”¿ç­–åˆ©å¥½çš„ç»†åˆ†è¡Œä¸šå’Œä¸šç»©ç¡®å®šæ€§è¾ƒé«˜çš„ä¼˜è´¨æ ‡çš„ã€‚"
        
        # ç¡®ä¿æ ¸å¿ƒæç¤ºè‡³å°‘200å­—
        if len(core_tip) < 200:
            core_tip += "ä¸æ­¤åŒæ—¶ï¼Œå…¨çƒç»æµå¤è‹æ€åŠ¿ä¾ç„¶å¤æ‚ï¼Œåœ°ç¼˜æ”¿æ²»é£é™©å’Œé€šèƒ€å‹åŠ›ä»éœ€å¯†åˆ‡å…³æ³¨ã€‚å›½å†…ç»æµéŸ§æ€§è¾ƒå¼ºï¼Œäº§ä¸šå‡çº§å’Œç§‘æŠ€åˆ›æ–°å°†ç»§ç»­æ¨åŠ¨ç»æµé«˜è´¨é‡å‘å±•ï¼Œä¸ºèµ„æœ¬å¸‚åœºæä¾›é•¿æœŸå¢é•¿åŠ¨åŠ›ã€‚æŠ•èµ„è€…åº”ä¿æŒç†æ€§ï¼Œæ ¹æ®è‡ªèº«é£é™©åå¥½å’ŒæŠ•èµ„ç›®æ ‡åˆ¶å®šåˆç†çš„æŠ•èµ„è®¡åˆ’ã€‚"
        
        return core_tip


class NewsGenerator:
    """
    è´¢ç»æ–°é—»ç”Ÿæˆå™¨

    è´Ÿè´£å°†å¤„ç†åçš„æ–°é—»ç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ–‡ä»¶ã€‚
    """
    
    @classmethod
    def generate_html(cls, processed_news: List[Dict[str, Any]], core_tip: str) -> str:
        """
        ç”Ÿæˆé€‚åˆå¾®ä¿¡å…¬ä¼—å·çš„çº¯HTMLæ ¼å¼ï¼Œæ— CSSç±»å’Œæ ·å¼æ ‡ç­¾
        """
        # ç”Ÿæˆå½“å‰æ—¥æœŸ
        today = datetime.now().strftime('%Y-%m-%d')
        
        # æ„å»ºçº¯HTMLå†…å®¹ï¼Œä¸ä½¿ç”¨ä»»ä½•CSSç±»ï¼Œåªä½¿ç”¨å†…è”æ ·å¼
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è´¢ç»æ–°é—» - {today}</title>
</head>
<body>
    <!-- æ ¸å¿ƒæç¤º -->
    <p style="margin:20px 0; font-size:16px; line-height:2;"><strong style="font-size:16px; color:#333;">æ ¸å¿ƒæç¤º</strong>ï¼š{core_tip}</p>
    
    <!-- æ–°é—»åˆ—è¡¨ -->
    {cls._generate_news_items(processed_news)}
</body>
</html>
        """
        
        return html_content
    
    @classmethod
    def _generate_news_items(cls, processed_news: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆæ–°é—»æ¡ç›®ï¼Œä½¿ç”¨çº¯HTMLå’Œå†…è”æ ·å¼ï¼Œå…¼å®¹å¾®ä¿¡å…¬ä¼—å·ç¼–è¾‘å™¨
        è¦æ±‚ï¼š1.æ‰€æœ‰å­—ä½“å¤§å°16 2.æ¯æ¡é—´ç©º1è¡Œ 3.æ‘˜è¦å’Œå…³è”åŸºé‡‘åŠ ç²— 4.åŒºåˆ†æ–°æ—§æ–°é—»
        """
        news_items_html = ''
        
        for i, news in enumerate(processed_news, 1):
            icon = news['icon']
            title = news['title']
            detail = news['detail']
            funds = news['related_funds']
            is_new = news.get('is_new', True)  # æ˜¯å¦ä¸ºæ–°æ–°é—»
            
            # æ—§æ–°é—»æ ‡é¢˜åæ·»åŠ ğŸ”„å›¾æ ‡æ³¨æ˜
            if not is_new:
                title += ' ğŸ”„'
            
            # ç”Ÿæˆå…³è”åŸºé‡‘éƒ¨åˆ†
            funds_content = ''
            if funds:
                funds_content = 'ã€'.join(funds)
            else:
                # å¦‚æœæ²¡æœ‰å…³è”åŸºé‡‘ï¼Œæ ¹æ®æ–°é—»å†…å®¹åŒ¹é…æœ€ç›¸å…³çš„åŸºé‡‘
                content = title + detail
                # å°è¯•åŒ¹é…åŸºé‡‘ç±»å‹
                matched_fund_type = None
                for fund_type, keywords in NewsProcessor.FUND_KEYWORDS.items():
                    if any(keyword in content for keyword in keywords):
                        matched_fund_type = fund_type
                        break
                # å¦‚æœåŒ¹é…åˆ°åŸºé‡‘ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤åŸºé‡‘
                if matched_fund_type and matched_fund_type in NewsProcessor.FUND_CODES:
                    default_funds = NewsProcessor.FUND_CODES[matched_fund_type][:2]
                    funds_content = 'ã€'.join(default_funds)
                else:
                    # ä½¿ç”¨é€šç”¨åŸºé‡‘
                    funds_content = 'äº‘è®¡ç®—ETF(516510)ã€å¤§æ•°æ®äº§ä¸šETF(516700)'
            
            # ç”Ÿæˆçº¯HTMLå’Œå†…è”æ ·å¼çš„æ–°é—»æ¡ç›®ï¼Œç¬¦åˆæ‰€æœ‰è¦æ±‚
            news_items_html += f"""
    <!-- æ–°é—»æ¡ç›® {i} -->
    <p style="margin:20px 0 10px 0; font-size:16px; font-weight:bold; color:#333; line-height:1.8;">{icon}{i}. {title}</p>
    <p style="margin:10px 0; font-size:16px; color:#555; line-height:1.8;"><strong>æ‘˜è¦</strong>ï¼š{detail}</p>
    <p style="margin:10px 0 20px 0; font-size:16px; color:#27ae60; line-height:1.8;"><strong>å…³è”åŸºé‡‘</strong>ï¼š{funds_content}</p>
            """
        
        return news_items_html
    
    @classmethod
    def save_html(cls, html_content: str, output_dir: str = ".") -> str:
        """
        ä¿å­˜HTMLæ–‡ä»¶
        """
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(output_dir, "index.html")  # æ”¹ä¸ºindex.html
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return output_path


def main() -> None:
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œè´¢ç»æ–°é—»è·å–ã€å¤„ç†å’Œç”Ÿæˆçš„å®Œæ•´æµç¨‹
    """
    print("=" * 50)
    print("è´¢ç»æ–°é—»çˆ¬å–ç”Ÿæˆå·¥å…·")
    print("=" * 50)
    
    try:
        logger.info("åˆå§‹åŒ–æ–°é—»è·å–å™¨...")
        
        # 1. è·å–è´¢ç»æ–°é—»
        all_news = NewsFetcher.get_finance_news(count=50)
        
        if not all_news:
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„è´¢ç»æ–°é—»")
            return
        
        logger.info(f"è·å–åˆ° {len(all_news)} æ¡æ–°é—»")
        
        # 2. å¤„ç†æ–°é—»æ•°æ®
        processed_news = NewsProcessor.process_news(all_news)
        core_tip = NewsProcessor.generate_core_tip(processed_news)
        
        logger.info(f"å¤„ç†åç”Ÿæˆ {len(processed_news)} æ¡æ–°é—»")
        
        # 3. ç”ŸæˆHTML
        html_content = NewsGenerator.generate_html(processed_news, core_tip)
        
        # 4. ä¿å­˜HTMLæ–‡ä»¶
        output_path = NewsGenerator.save_html(html_content)
        
        print("\n" + "=" * 50)
        print("ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        print(f"è´¢ç»æ–°é—»å·²ä¿å­˜è‡³: {output_path}")
        print("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¯¥æ–‡ä»¶æŸ¥çœ‹ï¼Œå¯ç›´æ¥å¤åˆ¶åˆ°å…¬ä¼—å·")
        print("=" * 50)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

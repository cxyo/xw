"""py
è´¢ç»æ–°é—»çˆ¬å–ç”Ÿæˆå·¥å…·

åŠŸèƒ½æè¿°:
    æœ¬è„šæœ¬ä»å¤šä¸ªè´¢ç»ç½‘ç«™æŠ“å–æœ€æ–°çš„è´¢ç»æ–°é—»ï¼Œ
    ç»è¿‡æ•°æ®å¤„ç†åç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ–‡ä»¶ã€‚

ä¸»è¦ç‰¹æ€§:
    1. æ”¯æŒä»å¤šä¸ªè´¢ç»ç½‘ç«™è·å–æ–°é—»
    2. è‡ªåŠ¨æå–æ–°é—»æ ¸å¿ƒä¿¡æ¯
    3. ç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ ¼å¼
    4. åŒ…å«åŸºé‡‘ç›¸å…³ä¿¡æ¯å’Œæ¨è
    5. æ”¯æŒè‡ªåŠ¨åŒ–æ›´æ–°

ä½¿ç”¨æ–¹å¼:
    ç›´æ¥è¿è¡Œ: python app.py
    ç”Ÿæˆçš„æ–‡ä»¶: finance_news.htmlï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰

ä½œè€…: Auto-generated
ç‰ˆæœ¬: 1.0
"""

import os
import re
import json
import logging
import random
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime

import requests
from bs4 import BeautifulSoup

# =============================================================================
# æ—¥å¿—é…ç½®
# =============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsFetcher:
    """
    è´¢ç»æ–°é—»è·å–å™¨

    è´Ÿè´£ä»å¤šä¸ªè´¢ç»ç½‘ç«™è·å–æœ€æ–°çš„è´¢ç»æ–°é—»ã€‚
    æ”¯æŒä¸œæ–¹è´¢å¯Œç½‘ã€æ–°æµªè´¢ç»ã€åŒèŠ±é¡ºè´¢ç»ç­‰ç½‘ç«™ã€‚
    """

    DEFAULT_HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    REQUEST_INTERVAL = 1.0

    _last_request_time = 0.0

    @classmethod
    def _ensure_request_interval(cls) -> None:
        """
        ç¡®ä¿è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«æœåŠ¡å™¨å°ç¦
        """
        current_time = datetime.now().timestamp()
        elapsed = current_time - cls._last_request_time
        if elapsed < cls.REQUEST_INTERVAL:
            import time
            time.sleep(cls.REQUEST_INTERVAL - elapsed)
        cls._last_request_time = datetime.now().timestamp()

    @classmethod
    def fetch_eastmoney_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»ä¸œæ–¹è´¢å¯Œç½‘è·å–è´¢ç»æ–°é—»
        """
        news_list = []
        try:
            # ä¿®æ”¹ä¸ºæ­£ç¡®çš„ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»URL
            url = "https://finance.eastmoney.com/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'  # ç¡®ä¿ç¼–ç æ­£ç¡®
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            news_items = soup.find_all(['h3', 'div'], class_=re.compile(r'(news|title)', re.I), limit=count*3)
            for item in news_items:
                a_elem = item.find('a')
                if a_elem and a_elem.get('href') and a_elem.get_text(strip=True):
                    title = a_elem.get_text(strip=True)
                    link = a_elem['href']
                    
                    # è¿‡æ»¤æ‰éæ–°é—»é“¾æ¥å’Œå¯¼èˆªé“¾æ¥
                    if len(title) < 10 or len(title) > 150:
                        continue
                    if any(keyword in link for keyword in ['javascript:', 'mailto:', '#', 'login', 'register']):
                        continue
                    
                    # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
                    if not link.startswith('http'):
                        if link.startswith('/'):
                            link = f"https://finance.eastmoney.com{link}"
                        else:
                            continue
                    
                    # åªä½¿ç”¨çˆ¬å–çš„æ‘˜è¦ï¼Œä¸ç”Ÿæˆ
                    detail = cls._get_news_detail(link)
                    
                    # ç¡®ä¿æ‘˜è¦å†…å®¹æ˜¯çœŸå®çˆ¬å–çš„ï¼Œä¸ç”Ÿæˆ
                    if not detail or len(detail) < 50:  # é™ä½é•¿åº¦è¦æ±‚ï¼Œç¡®ä¿ä½¿ç”¨çœŸå®å†…å®¹
                        # å¦‚æœçˆ¬å–åˆ°çš„å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨æ ‡é¢˜åŠ ä¸Šéƒ¨åˆ†æ­£æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
                        if detail:
                            # ä½¿ç”¨çˆ¬å–åˆ°çš„å…¨éƒ¨å†…å®¹
                            pass
                        else:
                            # å¦‚æœå®Œå…¨æ²¡æœ‰çˆ¬å–åˆ°å†…å®¹ï¼Œè·³è¿‡è¿™æ¡æ–°é—»
                            continue
                    
                    # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                    if len(detail) > 400:
                        # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                        detail = detail[:400]
                        # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                        for i in range(len(detail)-1, 150, -1):
                            if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                detail = detail[:i+1]
                                break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                        detail = detail[:400]
                    
                    news_list.append({
                        'title': title,
                        'link': link,
                        'source': 'ä¸œæ–¹è´¢å¯Œç½‘',
                        'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»å¤±è´¥: {str(e)}")
        return news_list

    @classmethod
    def fetch_sina_finance_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»æ–°æµªè´¢ç»è·å–è´¢ç»æ–°é—»
        """
        news_list = []
        try:
            url = "https://finance.sina.com.cn/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'  # ç¡®ä¿ç¼–ç æ­£ç¡®
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            # æ–°æµªè´¢ç»çš„æ–°é—»æ ‡é¢˜é€šå¸¸åœ¨h2ã€h3æˆ–ç‰¹å®šclassçš„divä¸­
            news_containers = soup.find_all(['h2', 'h3', 'div'], class_=re.compile(r'(news|title|article)', re.I), limit=count*3)
            for container in news_containers:
                a_elem = container.find('a', {'target': '_blank'})
                if a_elem and a_elem.get('href') and a_elem.get_text(strip=True):
                    title = a_elem.get_text(strip=True)
                    link = a_elem['href']
                    
                    # è¿‡æ»¤æ‰éæ–°é—»é“¾æ¥å’ŒçŸ­æ ‡é¢˜
                    if len(title) < 10 or len(title) > 150:
                        continue
                    if not link.startswith('http'):
                        continue
                    if any(keyword in link for keyword in ['javascript:', 'mailto:', '#', 'login', 'register', 'video']):
                        continue
                    
                    # è¿‡æ»¤å‡ºè´¢ç»ç›¸å…³æ–°é—»
                    finance_keywords = ['ç»æµ', 'è‚¡ç¥¨', 'åŸºé‡‘', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢', 'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'å€ºåˆ¸', 'ETF']
                    if any(keyword in title for keyword in finance_keywords):
                        # åªä½¿ç”¨çˆ¬å–çš„æ‘˜è¦ï¼Œä¸ç”Ÿæˆ
                        detail = cls._get_news_detail(link)
                        
                        # ç¡®ä¿æ‘˜è¦å†…å®¹æ˜¯çœŸå®çˆ¬å–çš„ï¼Œä¸ç”Ÿæˆ
                        if not detail or len(detail) < 50:  # é™ä½é•¿åº¦è¦æ±‚ï¼Œç¡®ä¿ä½¿ç”¨çœŸå®å†…å®¹
                            # å¦‚æœçˆ¬å–åˆ°çš„å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨æ ‡é¢˜åŠ ä¸Šéƒ¨åˆ†æ­£æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
                            if detail:
                                # ä½¿ç”¨çˆ¬å–åˆ°çš„å…¨éƒ¨å†…å®¹
                                pass
                            else:
                                # å¦‚æœå®Œå…¨æ²¡æœ‰çˆ¬å–åˆ°å†…å®¹ï¼Œè·³è¿‡è¿™æ¡æ–°é—»
                                continue
                        
                        # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                        if len(detail) > 400:
                            # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                            detail = detail[:400]
                            # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                            for i in range(len(detail)-1, 150, -1):
                                if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                    detail = detail[:i+1]
                                    break
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                            detail = detail[:400]
                        
                        news_list.append({
                            'title': title,
                            'link': link,
                            'source': 'æ–°æµªè´¢ç»',
                            'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                            'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                        
                        if len(news_list) >= count:
                            break
        except Exception as e:
            logger.error(f"è·å–æ–°æµªè´¢ç»æ–°é—»å¤±è´¥: {str(e)}")
        return news_list

    @classmethod
    def _generate_enhanced_summary(cls, title: str) -> str:
        """
        ç”Ÿæˆå¢å¼ºå‹æ‘˜è¦ï¼Œç¡®ä¿é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´ï¼Œä¸”ä¸æ ‡é¢˜å†…å®¹ä¸åŒ
        """
        # ä¸å†ä»¥æ ‡é¢˜ä½œä¸ºåŸºç¡€ï¼Œè€Œæ˜¯ç›´æ¥ç”Ÿæˆä¸æ ‡é¢˜ç›¸å…³ä½†ä¸åŒçš„æ‘˜è¦
        
        # æ ¹æ®ä¸åŒä¸»é¢˜ç”Ÿæˆæ‰©å±•å†…å®¹
        enhanced_content = ""
        
        if 'REITs' in title or 'ä¿ç§Ÿæˆ¿' in title:
            enhanced_content = "è¿‘æœŸï¼Œå…¬å‹ŸREITså¸‚åœºè¡¨ç°æ´»è·ƒï¼ŒäºŒçº§å¸‚åœºè¶…è·Œåå¼¹ï¼Œä¿ç§Ÿæˆ¿æ¿å—é¢†æ¶¨ï¼Œå¤šåªREITsäº§å“æ¶¨å¹…æ˜¾è‘—ã€‚åŒæ—¶ï¼Œå‘è¡Œå¸‚åœºä¿æŒçƒ­åº¦ï¼Œå¤šåªæ–°REITsäº§å“æ­£åœ¨ç­¹å¤‡ä¸­ã€‚åˆ†æäººå£«æŒ‡å‡ºï¼ŒREITsä½œä¸ºèµ„äº§é…ç½®çš„é‡è¦å·¥å…·ï¼Œå…·æœ‰ç¨³å®šç°é‡‘æµå’Œé•¿æœŸå¢å€¼æ½œåŠ›ï¼Œé€‚åˆé•¿æœŸæŠ•èµ„ã€‚"
        elif 'æ¸¯è‚¡' in title:
            enhanced_content = "å¤šå®¶æ¸¯è‚¡åŸºé‡‘è¿‘æœŸå¯†é›†å¤§å¹…æå‰ç»“å‹Ÿï¼Œåæ˜ äº†å¸‚åœºå¯¹æ¸¯è‚¡å¸‚åœºçš„çœ‹å¥½ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€å†…åœ°ä¸é¦™æ¸¯é‡‘èå¸‚åœºäº’è”äº’é€šä¸æ–­æ·±åŒ–ï¼Œæ¸¯è‚¡å¸‚åœºçš„æŠ•èµ„ä»·å€¼æ—¥ç›Šå‡¸æ˜¾ã€‚åœ¨å…¨çƒç»æµå¤è‹çš„èƒŒæ™¯ä¸‹ï¼Œæ¸¯è‚¡å¸‚åœºçš„ä¼˜è´¨ä¼ä¸šæœ‰æœ›è¿æ¥ä¼°å€¼ä¿®å¤å’Œä¸šç»©å¢é•¿çš„åŒé‡åˆ©å¥½ã€‚"
        elif 'åŸºé‡‘å…¬å¸' in title or 'è‚¡æƒ' in title:
            enhanced_content = "åŸºé‡‘è¡Œä¸šçš„è‚¡æƒå˜åŠ¨å’Œå¢èµ„å¼•æ–°æˆä¸ºå¸‚åœºå…³æ³¨ç„¦ç‚¹ã€‚é•¿å®‰åŸºé‡‘6.67%è‚¡æƒå†è½¬è®©ï¼Œåæ¶¦å…ƒå¤§åŸºé‡‘æ‹Ÿå¢èµ„å¼•å…¥æ–°è‚¡ä¸œï¼Œè¿™äº›å˜åŠ¨åæ˜ äº†åŸºé‡‘è¡Œä¸šçš„æ•´åˆè¶‹åŠ¿ã€‚ä¸šå†…äººå£«æŒ‡å‡ºï¼ŒåŸºé‡‘å…¬å¸é€šè¿‡è‚¡æƒè°ƒæ•´å’Œå¢èµ„æ‰©è‚¡ï¼Œå¯ä»¥å¢å¼ºèµ„æœ¬å®åŠ›ï¼Œæå‡æŠ•èµ„ç®¡ç†èƒ½åŠ›ã€‚"
        elif 'Aè‚¡' in title or 'è‚¡å¸‚' in title:
            enhanced_content = "Aè‚¡å¸‚åœºè¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œå¸‚åœºåšå¤šæƒ…ç»ªæµ“åšã€‚åŸºé‡‘ç»ç†ä»¬çº·çº·ç­›é€‰2026å¹´çš„'æœºé‡æ¸…å•'ï¼Œçœ‹å¥½é«˜æ™¯æ°”è¡Œä¸šçš„æŠ•èµ„æœºä¼šã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€ç»æµåŸºæœ¬é¢çš„é€æ­¥æ”¹å–„å’Œæ”¿ç­–æ”¯æŒåŠ›åº¦çš„åŠ å¤§ï¼ŒAè‚¡å¸‚åœºæœ‰æœ›è¿æ¥æ›´å¤šæŠ•èµ„æœºä¼šã€‚"
        elif 'ETF' in title:
            # æ ¹æ®æ ‡é¢˜ä¸­çš„å…·ä½“ETFç±»å‹ç”Ÿæˆä¸åŒçš„æ‘˜è¦
            if 'è§„æ¨¡' in title or 'å‡€æµå…¥' in title:
                enhanced_content = "è¿‘æœŸETFå¸‚åœºè§„æ¨¡æŒç»­æ‰©å¤§ï¼Œå¤šåªETFäº§å“è·å¾—èµ„é‡‘å‡€æµå…¥ã€‚å…¶ä¸­ï¼Œä¸­è¯500ETFã€æ²ªæ·±300ETFç­‰å®½åŸºETFè¡¨ç°å°¤ä¸ºçªå‡ºï¼Œå•æ—¥å‡€æµå…¥é‡‘é¢è¶…è¿‡æ•°åäº¿å…ƒã€‚ETFä½œä¸ºæŒ‡æ•°åŒ–æŠ•èµ„å·¥å…·ï¼Œå…·æœ‰äº¤æ˜“ä¾¿æ·ã€æˆæœ¬ä½ã€é€æ˜åº¦é«˜ç­‰ä¼˜åŠ¿ï¼Œå—åˆ°æŠ•èµ„è€…çš„é’çã€‚"
            elif 'è¡Œä¸šETF' in title or 'é£å‘æ ‡' in title:
                enhanced_content = "è¡Œä¸šETFå¸‚åœºè¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œä¸åŒè¡Œä¸šETFå‘ˆç°å·®å¼‚åŒ–èµ°åŠ¿ã€‚æœ‰è‰²é‡‘å±ETFã€åŒ–å·¥ETFç­‰å‘¨æœŸç±»ETFæ¶¨å¹…æ˜¾è‘—ï¼Œè€Œé¦™æ¸¯è¯åˆ¸ETFã€æ¸¯è‚¡é€šETFç­‰è·¨å¢ƒETFäº¤æŠ•æ´»è·ƒã€‚æŠ•èµ„è€…å¯é€šè¿‡è¡Œä¸šETFæŠŠæ¡ä¸åŒè¡Œä¸šçš„æŠ•èµ„æœºä¼šã€‚"
            elif 'å®½åŸº' in title or 'å…¨æ™¯å›¾' in title:
                enhanced_content = "å®½åŸºETFå¸‚åœºè¡¨ç°åˆ†åŒ–ï¼ŒåŒåˆ›ETFé¢†è·‘ä¸šç»©ï¼Œæ²ªæ·±300ETFä»æ˜¯èµ„é‡‘é’ççš„'å¸é‡‘ç‹'ã€‚æˆªè‡³ç›®å‰ï¼ŒETFæ€»è§„æ¨¡å¹´å†…å¢é•¿æ˜¾è‘—ï¼Œé€¼è¿‘ä¸‡äº¿å…ƒå¤§å…³ã€‚å®½åŸºETFä¸ºæŠ•èµ„è€…æä¾›äº†ä¾¿æ·çš„å¸‚åœºæ•´ä½“å¸ƒå±€å·¥å…·ã€‚"
            else:
                enhanced_content = "ETFå¸‚åœºè¿‘æœŸè¿æ¥çˆ†å‘å¼å¢é•¿ï¼Œå¤šåªETFäº§å“æ¶¨å¹…æ˜¾è‘—ã€‚åŸºé‡‘å…¬å¸ç«é€Ÿè§£è¯»è®¤ä¸ºï¼Œæ˜¥å­£èºåŠ¨è¡Œæƒ…æœ‰æœ›å»¶ç»­ï¼Œé™©èµ„å…¥åœºæˆ–æˆä¸ºå¸‚åœºä¸Šæ¶¨çš„åŠ åˆ†é¡¹ã€‚ETFä½œä¸ºæŒ‡æ•°åŒ–æŠ•èµ„å·¥å…·ï¼Œå…·æœ‰äº¤æ˜“ä¾¿æ·ã€æˆæœ¬ä½ã€é€æ˜åº¦é«˜ç­‰ä¼˜åŠ¿ï¼Œå—åˆ°æŠ•èµ„è€…çš„é’çã€‚"
        elif 'æ¶ˆè´¹' in title:
            enhanced_content = "æ¶ˆè´¹æ¿å—è¿‘æœŸè¡¨ç°å¼ºåŠ¿ï¼Œæˆä¸ºå¸‚åœºå…³æ³¨çš„ç„¦ç‚¹ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€å±…æ°‘æ”¶å…¥æ°´å¹³çš„æé«˜å’Œæ¶ˆè´¹å‡çº§çš„æ¨è¿›ï¼Œæ¶ˆè´¹è¡Œä¸šæœ‰æœ›ä¿æŒç¨³å®šå¢é•¿ã€‚æŠ•èµ„è€…å¯å…³æ³¨ç™½é…’ã€å®¶ç”µã€é£Ÿå“é¥®æ–™ç­‰ä¼ ç»Ÿæ¶ˆè´¹è¡Œä¸šï¼Œä»¥åŠç”µå•†ã€æ–°èƒ½æºæ±½è½¦ç­‰æ–°å…´æ¶ˆè´¹é¢†åŸŸã€‚"
        elif 'åŒ»è¯' in title:
            enhanced_content = "åŒ»è¯æ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œåˆ›æ–°è¯ã€åŒ»ç–—å™¨æ¢°ç­‰ç»†åˆ†é¢†åŸŸæ¶¨å¹…æ˜¾è‘—ã€‚éšç€äººå£è€é¾„åŒ–åŠ å‰§å’ŒåŒ»ç–—éœ€æ±‚çš„å¢é•¿ï¼ŒåŒ»è¯è¡Œä¸šé•¿æœŸæŠ•èµ„ä»·å€¼å‡¸æ˜¾ã€‚æŠ•èµ„è€…å¯å…³æ³¨åˆ›æ–°èƒ½åŠ›å¼ºã€ç ”å‘æŠ•å…¥é«˜çš„åŒ»è¯ä¼ä¸šï¼Œä»¥åŠå—ç›Šäºæ”¿ç­–æ”¯æŒçš„åŒ»è¯ç»†åˆ†é¢†åŸŸã€‚"
        elif 'æ–°èƒ½æº' in title or 'å…‰ä¼' in title or 'é£ç”µ' in title:
            enhanced_content = "æ–°èƒ½æºæ¿å—è¿‘æœŸè¡¨ç°å¼ºåŠ¿ï¼Œå…‰ä¼ã€é£ç”µç­‰ç»†åˆ†é¢†åŸŸæ¶¨å¹…æ˜¾è‘—ã€‚éšç€å…¨çƒèƒ½æºè½¬å‹çš„æ¨è¿›ï¼Œæ–°èƒ½æºè¡Œä¸šè¿æ¥äº†å¿«é€Ÿå‘å±•çš„æœºé‡ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œæ–°èƒ½æºè¡Œä¸šå…·æœ‰å¹¿é˜”çš„å‘å±•ç©ºé—´ï¼ŒæŠ•èµ„è€…å¯å…³æ³¨å…‰ä¼ã€é£ç”µã€å‚¨èƒ½ç­‰ç»†åˆ†é¢†åŸŸçš„æŠ•èµ„æœºä¼šã€‚"
        elif 'ç§‘æŠ€' in title or 'äººå·¥æ™ºèƒ½' in title:
            enhanced_content = "ç§‘æŠ€æ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œäººå·¥æ™ºèƒ½ã€èŠ¯ç‰‡ç­‰ç»†åˆ†é¢†åŸŸæ¶¨å¹…æ˜¾è‘—ã€‚éšç€ç§‘æŠ€çš„ä¸æ–­è¿›æ­¥å’Œåº”ç”¨åœºæ™¯çš„æ‹“å±•ï¼Œç§‘æŠ€è¡Œä¸šé•¿æœŸæŠ•èµ„ä»·å€¼å‡¸æ˜¾ã€‚æŠ•èµ„è€…å¯å…³æ³¨äººå·¥æ™ºèƒ½ã€èŠ¯ç‰‡ã€äº‘è®¡ç®—ç­‰å‰æ²¿ç§‘æŠ€é¢†åŸŸï¼Œä»¥åŠå—ç›Šäºæ•°å­—åŒ–è½¬å‹çš„ä¼ ç»Ÿè¡Œä¸šã€‚"
        elif 'èŠ¯ç‰‡' in title or 'åŠå¯¼ä½“' in title:
            enhanced_content = "èŠ¯ç‰‡æ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œå—åˆ°å¸‚åœºå¹¿æ³›å…³æ³¨ã€‚éšç€å…¨çƒèŠ¯ç‰‡çŸ­ç¼ºé—®é¢˜çš„ç¼“è§£å’ŒåŠå¯¼ä½“äº§ä¸šçš„å‡çº§ï¼ŒèŠ¯ç‰‡è¡Œä¸šè¿æ¥äº†æ–°çš„å‘å±•æœºé‡ã€‚åˆ†æäººå£«è®¤ä¸ºï¼ŒèŠ¯ç‰‡ä½œä¸ºç§‘æŠ€äº§ä¸šçš„æ ¸å¿ƒéƒ¨ä»¶ï¼Œå…¶å¸‚åœºéœ€æ±‚å°†æŒç»­å¢é•¿ï¼Œå°¤å…¶æ˜¯åœ¨äººå·¥æ™ºèƒ½ã€5Gã€æ–°èƒ½æºæ±½è½¦ç­‰é¢†åŸŸã€‚"
        elif 'äº‘è®¡ç®—' in title or 'äº‘æœåŠ¡' in title:
            enhanced_content = "äº‘è®¡ç®—æ¿å—è¿‘æœŸè¡¨ç°å¼ºåŠ²ï¼Œå¸‚åœºå…³æ³¨åº¦è¾ƒé«˜ã€‚éšç€æ•°å­—åŒ–è½¬å‹çš„æ¨è¿›å’Œä¼ä¸šä¸Šäº‘éœ€æ±‚çš„å¢åŠ ï¼Œäº‘è®¡ç®—è¡Œä¸šæœ‰æœ›ä¿æŒå¿«é€Ÿå¢é•¿ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œäº‘è®¡ç®—ä½œä¸ºæ•°å­—ç»æµçš„åŸºç¡€è®¾æ–½ï¼Œå…¶å¸‚åœºè§„æ¨¡å°†æŒç»­æ‰©å¤§ï¼Œå°¤å…¶æ˜¯åœ¨äººå·¥æ™ºèƒ½ã€å¤§æ•°æ®ç­‰é¢†åŸŸçš„åº”ç”¨ä¸æ–­æ·±åŒ–ã€‚"
        elif 'å¤§æ•°æ®' in title or 'æ•°æ®è¦ç´ ' in title:
            enhanced_content = "å¤§æ•°æ®æ¿å—è¿‘æœŸå—åˆ°å¸‚åœºå…³æ³¨ï¼Œæ•°æ®è¦ç´ å¸‚åœºåŒ–æ”¹é©çš„æ¨è¿›ä¸ºè¡Œä¸šå¸¦æ¥äº†æ–°çš„å‘å±•æœºé‡ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€æ•°æ®æˆä¸ºé‡è¦çš„ç”Ÿäº§è¦ç´ ï¼Œå¤§æ•°æ®äº§ä¸šçš„å¸‚åœºè§„æ¨¡å°†æŒç»­æ‰©å¤§ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é‡‡é›†ã€å­˜å‚¨ã€åˆ†æå’Œåº”ç”¨ç­‰ç¯èŠ‚ã€‚"
        elif 'é‡‘èç§‘æŠ€' in title or 'æ•°å­—é‡‘è' in title:
            enhanced_content = "é‡‘èç§‘æŠ€æ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œæ•°å­—é‡‘èçš„å‘å±•ä¸ºé‡‘èè¡Œä¸šå¸¦æ¥äº†æ–°çš„å˜é©ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€é‡‘èç§‘æŠ€çš„ä¸æ–­åˆ›æ–°å’Œåº”ç”¨ï¼Œé‡‘èæœåŠ¡çš„æ•ˆç‡å’Œè´¨é‡å°†å¾—åˆ°æå‡ï¼ŒåŒæ—¶ä¹Ÿå°†å¸¦æ¥æ–°çš„æŠ•èµ„æœºä¼šã€‚"
        elif 'æ±½è½¦' in title or 'æ–°èƒ½æºæ±½è½¦' in title:
            enhanced_content = "æ±½è½¦æ¿å—è¿‘æœŸè¡¨ç°å¼ºåŠ¿ï¼Œå°¤å…¶æ˜¯æ–°èƒ½æºæ±½è½¦é¢†åŸŸã€‚éšç€å…¨çƒæ±½è½¦äº§ä¸šçš„ç”µåŠ¨åŒ–è½¬å‹ï¼Œæ–°èƒ½æºæ±½è½¦å¸‚åœºè§„æ¨¡æŒç»­æ‰©å¤§ï¼Œç›¸å…³äº§ä¸šé“¾ä¼ä¸šå—ç›Šæ˜æ˜¾ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œæ–°èƒ½æºæ±½è½¦è¡Œä¸šçš„å‘å±•å°†å¸¦åŠ¨ç”µæ± ã€ç”µæœºã€ç”µæ§ç­‰ä¸Šä¸‹æ¸¸äº§ä¸šé“¾çš„å‘å±•ã€‚"
        elif 'æ¸¸æˆ' in title or 'ç”µç«' in title:
            enhanced_content = "æ¸¸æˆæ¿å—è¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œç”µç«äº§ä¸šçš„å¿«é€Ÿå‘å±•ä¸ºè¡Œä¸šå¸¦æ¥äº†æ–°çš„å¢é•¿åŠ¨åŠ›ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€æ¸¸æˆè¡Œä¸šçš„å†…å®¹åˆ›æ–°å’ŒæŠ€æœ¯å‡çº§ï¼Œä»¥åŠç”µç«å¸‚åœºçš„ä¸æ–­æ‰©å¤§ï¼Œæ¸¸æˆè¡Œä¸šçš„å¸‚åœºè§„æ¨¡å°†æŒç»­å¢é•¿ã€‚"
        elif 'é«˜è‚¡æ¯' in title or 'çº¢åˆ©' in title:
            enhanced_content = "é«˜è‚¡æ¯æ¿å—è¿‘æœŸå—åˆ°å¸‚åœºå…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨å¸‚åœºæ³¢åŠ¨è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œé«˜è‚¡æ¯è‚¡ç¥¨çš„é˜²å¾¡æ€§ä¼˜åŠ¿å‡¸æ˜¾ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œé«˜è‚¡æ¯è‚¡ç¥¨å…·æœ‰ç¨³å®šçš„ç°é‡‘æµå’Œè‰¯å¥½çš„åˆ†çº¢èƒ½åŠ›ï¼Œé€‚åˆé•¿æœŸæŠ•èµ„å’Œä»·å€¼æŠ•èµ„ã€‚"
        elif 'åŸºé‡‘å…¬å¸' in title or 'è‚¡æƒ' in title or 'è½¬è®©' in title:
            enhanced_content = "åŸºé‡‘è¡Œä¸šçš„è‚¡æƒå˜åŠ¨å’Œå¢èµ„å¼•æ–°æˆä¸ºå¸‚åœºå…³æ³¨ç„¦ç‚¹ã€‚è¿‘æœŸå¤šå®¶åŸºé‡‘å…¬å¸å‘å¸ƒè‚¡æƒå˜åŠ¨å…¬å‘Šï¼Œè¿™äº›å˜åŠ¨åæ˜ äº†åŸºé‡‘è¡Œä¸šçš„æ•´åˆè¶‹åŠ¿ã€‚ä¸šå†…äººå£«æŒ‡å‡ºï¼ŒåŸºé‡‘å…¬å¸é€šè¿‡è‚¡æƒè°ƒæ•´å’Œå¢èµ„æ‰©è‚¡ï¼Œå¯ä»¥å¢å¼ºèµ„æœ¬å®åŠ›ï¼Œæå‡æŠ•èµ„ç®¡ç†èƒ½åŠ›ã€‚"
        elif 'æ¸…ç›˜' in title or 'è§„æ¨¡' in title or 'è¿·ä½ ' in title:
            enhanced_content = "è¿‘æœŸå¤šåªåŸºé‡‘å‘å¸ƒæ¸…ç›˜é¢„è­¦ï¼Œéƒ¨åˆ†ç»©ä¼˜åŸºé‡‘ä¹Ÿé­é‡è§„æ¨¡'è¿·ä½ 'çš„å°´å°¬ã€‚åˆ†æäººå£«è®¤ä¸ºï¼ŒåŸºé‡‘è§„æ¨¡çš„å˜åŒ–å—åˆ°å¤šç§å› ç´ å½±å“ï¼ŒåŒ…æ‹¬å¸‚åœºç¯å¢ƒã€æŠ•èµ„è€…åå¥½å’ŒåŸºé‡‘ç»ç†çš„æŠ•èµ„ä¸šç»©ç­‰ã€‚æŠ•èµ„è€…åœ¨é€‰æ‹©åŸºé‡‘æ—¶ï¼Œåº”ç»¼åˆè€ƒè™‘åŸºé‡‘çš„ä¸šç»©è¡¨ç°ã€åŸºé‡‘ç»ç†çš„ç®¡ç†èƒ½åŠ›å’ŒåŸºé‡‘å…¬å¸çš„æ•´ä½“å®åŠ›ã€‚"
        elif 'FOF' in title or 'åŸºé‡‘ä¸­åŸºé‡‘' in title:
            enhanced_content = "FOFåŸºé‡‘è¿‘æœŸå—åˆ°å¸‚åœºå…³æ³¨ï¼Œéƒ¨åˆ†FOFåŸºé‡‘ä¸€æ—¥ç»“å‹Ÿï¼Œåæ˜ äº†æŠ•èµ„è€…å¯¹FOFäº§å“çš„è®¤å¯ã€‚FOFåŸºé‡‘é€šè¿‡åˆ†æ•£æŠ•èµ„äºå¤šåªåŸºé‡‘ï¼Œé™ä½äº†å•ä¸€åŸºé‡‘çš„é£é™©ï¼Œé€‚åˆé£é™©åå¥½è¾ƒä½çš„æŠ•èµ„è€…ã€‚åˆ†æäººå£«è®¤ä¸ºï¼ŒFOFåŸºé‡‘å°†æˆä¸ºæœªæ¥åŸºé‡‘å¸‚åœºçš„é‡è¦å‘å±•æ–¹å‘ã€‚"
        elif 'è‘›å…°' in title or 'å‘¨è”šæ–‡' in title or 'åŸºé‡‘ç»ç†' in title:
            enhanced_content = "æ˜æ˜ŸåŸºé‡‘ç»ç†çš„åŠ¨å‘å—åˆ°å¸‚åœºå¹¿æ³›å…³æ³¨ã€‚è¿‘æœŸè‘›å…°ã€å‘¨è”šæ–‡ç­‰çŸ¥ååŸºé‡‘ç»ç†ç®¡ç†çš„åŸºé‡‘å‡ºç°æ–°åŠ¨æ€ï¼Œè¿™äº›å˜åŒ–å¯èƒ½åæ˜ äº†åŸºé‡‘ç»ç†å¯¹å¸‚åœºçš„åˆ¤æ–­å’ŒæŠ•èµ„ç­–ç•¥çš„è°ƒæ•´ã€‚æŠ•èµ„è€…åœ¨å…³æ³¨æ˜æ˜ŸåŸºé‡‘ç»ç†çš„åŒæ—¶ï¼Œä¹Ÿåº”ç†æ€§çœ‹å¾…åŸºé‡‘çš„é•¿æœŸä¸šç»©è¡¨ç°ã€‚"
        elif 'è´¹ç‡' in title or 'æ”¹é©' in title or 'è®©åˆ©' in title:
            enhanced_content = "åŸºé‡‘è´¹ç‡æ”¹é©æ˜¯è¿‘æœŸåŸºé‡‘å¸‚åœºçš„é‡è¦è¯é¢˜ã€‚å…¬å‹ŸåŸºé‡‘è´¹ç‡æ”¹é©çš„å®æ–½ï¼Œå°†ä¸ºæŠ•èµ„è€…å¸¦æ¥å®å®åœ¨åœ¨çš„å¥½å¤„ï¼Œæ¯å¹´è®©åˆ©è¶…500äº¿å…ƒã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œè´¹ç‡æ”¹é©å°†æ¨åŠ¨åŸºé‡‘è¡Œä¸šå‘æ›´è§„èŒƒã€æ›´é€æ˜çš„æ–¹å‘å‘å±•ï¼Œæœ‰åˆ©äºæå‡æŠ•èµ„è€…çš„è·å¾—æ„Ÿã€‚"
        elif 'å¼€é—¨çº¢' in title or 'æ¶¨' in title or 'æ”¶ç›Šç‡' in title:
            enhanced_content = "è¿‘æœŸAè‚¡å¸‚åœºå–œè¿å¼€é—¨çº¢ï¼ŒåŸºé‡‘å¸‚åœºä¹Ÿè¡¨ç°æ´»è·ƒï¼Œå¤šåªåŸºé‡‘æ¶¨å¹…æ˜¾è‘—ã€‚å¸‚åœºåšå¤šæƒ…ç»ªæµ“åšï¼ŒæŠ•èµ„è€…å¯¹2026å¹´çš„å¸‚åœºè¡¨ç°å……æ»¡æœŸå¾…ã€‚åˆ†æäººå£«è®¤ä¸ºï¼Œéšç€ç»æµåŸºæœ¬é¢çš„é€æ­¥æ”¹å–„å’Œæ”¿ç­–æ”¯æŒåŠ›åº¦çš„åŠ å¤§ï¼ŒåŸºé‡‘å¸‚åœºæœ‰æœ›è¿æ¥æ›´å¤šæŠ•èµ„æœºä¼šã€‚"
        else:
            # ä¸ºä¸åŒç±»å‹çš„æ–°é—»ç”Ÿæˆä¸åŒçš„é»˜è®¤æ‘˜è¦ï¼Œé¿å…é‡å¤
            if 'ETF' in title:
                enhanced_content = "ETFå¸‚åœºè¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œå¤šåªETFäº§å“æ¶¨å¹…æ˜¾è‘—ã€‚ETFä½œä¸ºæŒ‡æ•°åŒ–æŠ•èµ„å·¥å…·ï¼Œå…·æœ‰äº¤æ˜“ä¾¿æ·ã€æˆæœ¬ä½ã€é€æ˜åº¦é«˜ç­‰ä¼˜åŠ¿ï¼Œå—åˆ°æŠ•èµ„è€…çš„é’çã€‚è¿‘æœŸETFå¸‚åœºè§„æ¨¡æŒç»­æ‰©å¤§ï¼Œåæ˜ äº†æŠ•èµ„è€…å¯¹æŒ‡æ•°åŒ–æŠ•èµ„çš„è®¤å¯ã€‚"
            elif 'åŸºé‡‘' in title:
                enhanced_content = "è¿‘æœŸåŸºé‡‘å¸‚åœºè¡¨ç°æ´»è·ƒï¼Œå„ç±»å‹åŸºé‡‘å‘ˆç°ä¸åŒçš„è¡¨ç°æ€åŠ¿ã€‚æŠ•èµ„è€…åº”æ ¹æ®è‡ªèº«çš„é£é™©åå¥½å’ŒæŠ•èµ„ç›®æ ‡ï¼Œé€‰æ‹©é€‚åˆè‡ªå·±çš„åŸºé‡‘äº§å“ã€‚åœ¨å¸‚åœºæ³¢åŠ¨è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œåˆ†æ•£æŠ•èµ„ã€é•¿æœŸæŒæœ‰æ˜¯è¾ƒä¸ºç¨³å¥çš„æŠ•èµ„æ–¹å¼ã€‚"
            else:
                enhanced_content = "è¿‘æœŸï¼Œé‡‘èå¸‚åœºè¡¨ç°æ´»è·ƒï¼Œå„æ¿å—è½®åŠ¨æ˜æ˜¾ã€‚åŸºé‡‘å¸‚åœºä¹Ÿå—åˆ°å½±å“ï¼Œç›¸å…³åŸºé‡‘äº§å“è¡¨ç°å„å¼‚ã€‚æŠ•èµ„è€…åº”ä¿æŒç†æ€§ï¼Œå…³æ³¨å¸‚åœºåŠ¨æ€ï¼Œæ ¹æ®è‡ªèº«é£é™©åå¥½åˆ¶å®šåˆç†çš„æŠ•èµ„ç­–ç•¥ã€‚"
        
        # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
        if len(enhanced_content) < 150:
            # ç»§ç»­æ·»åŠ å†…å®¹ï¼Œç¡®ä¿é•¿åº¦è¶³å¤Ÿ
            enhanced_content += " å¸‚åœºåˆ†æäººå£«æŒ‡å‡ºï¼Œå½“å‰å¸‚åœºç¯å¢ƒä¸‹ï¼ŒæŠ•èµ„è€…åº”å…³æ³¨æ”¿ç­–é¢çš„å˜åŒ–å’Œç»æµåŸºæœ¬é¢çš„æ”¹å–„ï¼ŒæŠŠæ¡ç»“æ„æ€§æŠ•èµ„æœºä¼šã€‚åŒæ—¶ï¼Œè¦æ³¨æ„æ§åˆ¶é£é™©ï¼Œé¿å…ç›²ç›®è·Ÿé£å’Œè¿½æ¶¨æ€è·Œã€‚"
        
        # å¦‚æœå†…å®¹è¿‡é•¿ï¼Œæˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
        if len(enhanced_content) > 400:
            enhanced_content = enhanced_content[:400]
            # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
            for i in range(len(enhanced_content)-1, 150, -1):
                if enhanced_content[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                    enhanced_content = enhanced_content[:i+1]
                    break
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
            enhanced_content = enhanced_content[:400]
        
        return enhanced_content
    
    @classmethod
    def _get_news_detail(cls, url: str) -> str:
        """
        è·å–æ–°é—»è¯¦æƒ…ï¼Œå¹¶æ§åˆ¶åœ¨150-400å­—ä¹‹é—´
        """
        try:
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ­£æ–‡å†…å®¹
            content = ''
            # å°è¯•å¤šç§å¸¸è§çš„æ­£æ–‡å®¹å™¨ç±»å
            content_selectors = [
                'div.art_context_box',  # ä¸œæ–¹è´¢å¯Œç½‘
                'div.article',  # æ–°æµªè´¢ç»
                'div.content',
                'div.main-content',
                'article',
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ç§»é™¤å¹¿å‘Šå’Œæ— ç”¨å…ƒç´ 
                    for ad in content_elem.find_all(['script', 'style', 'div', 'span'], class_=re.compile(r'(ad|advert|promo|æ¨è|ç›¸å…³|åˆ†äº«)', re.I)):
                        ad.decompose()
                    content = content_elem.get_text(strip=True, separator='\n')
                    break
            
            # ç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
            content = ' '.join(content.split())
            
            # æ§åˆ¶æ‘˜è¦é•¿åº¦åœ¨150-400å­—ä¹‹é—´
            if len(content) < 150:
                # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œè¿”å›åŸå†…å®¹
                return content
            elif len(content) > 400:
                # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªå–400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                content = content[:400]
                # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                for i in range(len(content)-1, 150, -1):
                    if content[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                        content = content[:i+1]
                        break
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                return content[:400]
            else:
                # å†…å®¹é•¿åº¦åˆé€‚ï¼Œç›´æ¥è¿”å›
                return content
        except Exception as e:
            logger.error(f"è·å–æ–°é—»è¯¦æƒ…å¤±è´¥ {url}: {str(e)}")
            return ""

    @classmethod
    def fetch_nbd_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»æ¯æ—¥ç»æµæ–°é—»åŸºé‡‘é¢‘é“è·å–æ–°é—»
        URL: https://money.nbd.com.cn/columns/440/
        ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
        """
        news_list = []
        try:
            url = "https://money.nbd.com.cn/columns/440/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼ŒæŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾
            all_links = soup.find_all('a', href=True, limit=100)
            for link in all_links:
                title = link.get_text(strip=True)
                href = link['href']
                
                # è¿‡æ»¤æ¡ä»¶
                if len(title) < 15 or len(title) > 150:  # æ”¾å®½æ ‡é¢˜é•¿åº¦è¦æ±‚
                    continue
                if not href.startswith('http'):
                    continue
                if 'javascript:' in href or '#' in href:
                    continue
                
                # åªä¿ç•™åŒ…å«åŸºé‡‘ç›¸å…³å…³é”®è¯çš„æ–°é—»
                fund_keywords = ['åŸºé‡‘', 'ETF', 'è‚¡ç¥¨', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢']
                if any(keyword in title for keyword in fund_keywords):
                    # åªä½¿ç”¨çˆ¬å–çš„æ‘˜è¦ï¼Œä¸ç”Ÿæˆ
                    detail = cls._get_news_detail(href)
                    
                    # ç¡®ä¿æ‘˜è¦å†…å®¹æ˜¯çœŸå®çˆ¬å–çš„ï¼Œä¸ç”Ÿæˆ
                    if not detail or len(detail) < 50:  # é™ä½é•¿åº¦è¦æ±‚ï¼Œç¡®ä¿ä½¿ç”¨çœŸå®å†…å®¹
                        # å¦‚æœçˆ¬å–åˆ°çš„å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨æ ‡é¢˜åŠ ä¸Šéƒ¨åˆ†æ­£æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
                        if detail:
                            # ä½¿ç”¨çˆ¬å–åˆ°çš„å…¨éƒ¨å†…å®¹
                            pass
                        else:
                            # å¦‚æœå®Œå…¨æ²¡æœ‰çˆ¬å–åˆ°å†…å®¹ï¼Œè·³è¿‡è¿™æ¡æ–°é—»
                            continue
                    
                    # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                    if len(detail) > 400:
                        # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                        detail = detail[:400]
                        # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                        for i in range(len(detail)-1, 150, -1):
                            if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                detail = detail[:i+1]
                                break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                        detail = detail[:400]
                    
                    news_list.append({
                        'title': title,
                        'link': href,
                        'source': 'æ¯æ—¥ç»æµæ–°é—»',
                        'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–æ¯æ—¥ç»æµæ–°é—»å¤±è´¥: {str(e)}")
        return news_list
    
    @classmethod
    def fetch_10jqka_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»åŒèŠ±é¡ºè´¢ç»åŸºé‡‘é¢‘é“è·å–æ–°é—»
        URL: https://m.10jqka.com.cn/fund/jjzx_list/
        ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
        """
        news_list = []
        try:
            url = "https://m.10jqka.com.cn/fund/jjzx_list/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼ŒæŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾
            all_links = soup.find_all('a', href=True, limit=100)
            for link in all_links:
                title = link.get_text(strip=True)
                href = link['href']
                
                # è¿‡æ»¤æ¡ä»¶
                if len(title) < 15 or len(title) > 150:
                    continue
                if not href.startswith('http'):
                    continue
                if 'javascript:' in href or '#' in href:
                    continue
                
                # åªä¿ç•™åŒ…å«åŸºé‡‘ç›¸å…³å…³é”®è¯çš„æ–°é—»
                fund_keywords = ['åŸºé‡‘', 'ETF', 'è‚¡ç¥¨', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢']
                if any(keyword in title for keyword in fund_keywords):
                    # åªä½¿ç”¨çˆ¬å–çš„æ‘˜è¦ï¼Œä¸ç”Ÿæˆ
                    detail = cls._get_news_detail(href)
                    
                    # ç¡®ä¿æ‘˜è¦å†…å®¹æ˜¯çœŸå®çˆ¬å–çš„ï¼Œä¸ç”Ÿæˆ
                    if not detail or len(detail) < 50:  # é™ä½é•¿åº¦è¦æ±‚ï¼Œç¡®ä¿ä½¿ç”¨çœŸå®å†…å®¹
                        # å¦‚æœçˆ¬å–åˆ°çš„å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨æ ‡é¢˜åŠ ä¸Šéƒ¨åˆ†æ­£æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
                        if detail:
                            # ä½¿ç”¨çˆ¬å–åˆ°çš„å…¨éƒ¨å†…å®¹
                            pass
                        else:
                            # å¦‚æœå®Œå…¨æ²¡æœ‰çˆ¬å–åˆ°å†…å®¹ï¼Œè·³è¿‡è¿™æ¡æ–°é—»
                            continue
                    
                    # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                    if len(detail) > 400:
                        # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                        detail = detail[:400]
                        # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                        for i in range(len(detail)-1, 150, -1):
                            if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                detail = detail[:i+1]
                                break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                        detail = detail[:400]
                    
                    news_list.append({
                        'title': title,
                        'link': href,
                        'source': 'åŒèŠ±é¡ºè´¢ç»',
                        'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–åŒèŠ±é¡ºè´¢ç»æ–°é—»å¤±è´¥: {str(e)}")
        return news_list
    
    @classmethod
    def fetch_dayfund_news(cls, count: int = 20) -> List[Dict[str, Any]]:
        """
        ä»åŸºé‡‘é€ŸæŸ¥ç½‘è·å–æ–°é—»
        URL: https://www.dayfund.cn/news/
        ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
        """
        news_list = []
        try:
            url = "https://www.dayfund.cn/news/"
            cls._ensure_request_interval()
            response = requests.get(url, headers=cls.DEFAULT_HEADERS, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼ŒæŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾
            all_links = soup.find_all('a', href=True, limit=100)
            for link in all_links:
                title = link.get_text(strip=True)
                href = link['href']
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if not href.startswith('http'):
                    if href.startswith('/'):
                        href = f"https://www.dayfund.cn{href}"
                    else:
                        continue
                
                # è¿‡æ»¤æ¡ä»¶
                if len(title) < 15 or len(title) > 150:
                    continue
                if 'javascript:' in href or '#' in href:
                    continue
                
                # åªä¿ç•™åŒ…å«åŸºé‡‘ç›¸å…³å…³é”®è¯çš„æ–°é—»
                fund_keywords = ['åŸºé‡‘', 'ETF', 'è‚¡ç¥¨', 'é‡‘è', 'å¸‚åœº', 'æŠ•èµ„', 'ç†è´¢']
                if any(keyword in title for keyword in fund_keywords):
                    # åªä½¿ç”¨çˆ¬å–çš„æ‘˜è¦ï¼Œä¸ç”Ÿæˆ
                    detail = cls._get_news_detail(href)
                    
                    # ç¡®ä¿æ‘˜è¦å†…å®¹æ˜¯çœŸå®çˆ¬å–çš„ï¼Œä¸ç”Ÿæˆ
                    if not detail or len(detail) < 50:  # é™ä½é•¿åº¦è¦æ±‚ï¼Œç¡®ä¿ä½¿ç”¨çœŸå®å†…å®¹
                        # å¦‚æœçˆ¬å–åˆ°çš„å†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨æ ‡é¢˜åŠ ä¸Šéƒ¨åˆ†æ­£æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
                        if detail:
                            # ä½¿ç”¨çˆ¬å–åˆ°çš„å…¨éƒ¨å†…å®¹
                            pass
                        else:
                            # å¦‚æœå®Œå…¨æ²¡æœ‰çˆ¬å–åˆ°å†…å®¹ï¼Œè·³è¿‡è¿™æ¡æ–°é—»
                            continue
                    
                    # ç¡®ä¿æ‘˜è¦é•¿åº¦åœ¨150åˆ°400å­—ä¹‹é—´
                    if len(detail) > 400:
                        # æˆªå–åˆ°400å­—å¹¶ç¡®ä¿å¥å­å®Œæ•´
                        detail = detail[:400]
                        # å°è¯•åœ¨å¥å­ç»“æŸå¤„æˆªæ–­
                        for i in range(len(detail)-1, 150, -1):
                            if detail[i] in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                                detail = detail[:i+1]
                                break
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç»“æŸç¬¦ï¼Œç›´æ¥æˆªå–400å­—
                        detail = detail[:400]
                    
                    news_list.append({
                        'title': title,
                        'link': href,
                        'source': 'åŸºé‡‘é€ŸæŸ¥ç½‘',
                        'detail': detail,  # å®Œæ•´æ˜¾ç¤ºæ‘˜è¦ï¼Œä¸åŠ ...
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
                    if len(news_list) >= count:
                        break
        except Exception as e:
            logger.error(f"è·å–åŸºé‡‘é€ŸæŸ¥ç½‘æ–°é—»å¤±è´¥: {str(e)}")
        return news_list
    
    @classmethod
    def get_finance_news(cls, count: int = 100) -> List[Dict[str, Any]]:
        """
        è·å–ç»¼åˆè´¢ç»æ–°é—»ï¼Œç¡®ä¿è·å–è¶³å¤Ÿæ•°é‡ï¼Œä¸ä½¿ç”¨é»˜è®¤æ•°æ®
        ä»5ä¸ªæ¥æºè·å–ï¼šä¸œæ–¹è´¢å¯Œç½‘ã€æ–°æµªè´¢ç»ã€æ¯æ—¥ç»æµæ–°é—»ã€åŒèŠ±é¡ºè´¢ç»ã€åŸºé‡‘é€ŸæŸ¥ç½‘
        """
        logger.info("æ­£åœ¨è·å–è´¢ç»æ–°é—»...")
        
        # ä»5ä¸ªæ¥æºè·å–æ–°é—»ï¼Œå¢åŠ è·å–æ•°é‡
        eastmoney_news = cls.fetch_eastmoney_news(count)
        sina_news = cls.fetch_sina_finance_news(count)
        nbd_news = cls.fetch_nbd_news(count)
        jqka_news = cls.fetch_10jqka_news(count)
        dayfund_news = cls.fetch_dayfund_news(count)
        
        # åˆå¹¶æ‰€æœ‰æ–°é—»
        all_news = eastmoney_news + sina_news + nbd_news + jqka_news + dayfund_news
        
        logger.info(f"æ€»å…±è·å–åˆ° {len(all_news)} æ¡åŸå§‹æ–°é—»")
        
        # ä¸¥æ ¼å»é‡ï¼Œä½¿ç”¨å®Œæ•´æ ‡é¢˜
        seen_titles = set()
        unique_news = []
        for news in all_news:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        logger.info(f"å»é‡åå‰©ä½™ {len(unique_news)} æ¡æ–°é—»")
        
        # æ‰“ä¹±é¡ºåº
        random.shuffle(unique_news)
        
        # ç¡®ä¿è¿”å›è¶³å¤Ÿæ•°é‡çš„æ–°é—»ï¼Œä¸ä½¿ç”¨é»˜è®¤æ•°æ®
        return unique_news[:count]


class NewsProcessor:
    """
    è´¢ç»æ–°é—»å¤„ç†å™¨

    è´Ÿè´£æ–°é—»æ•°æ®çš„å¤„ç†ã€åˆ†ç±»å’ŒåŸºé‡‘å…³è”ã€‚
    """
    
    # åŸºé‡‘å…³è”å…³é”®è¯æ˜ å°„
    FUND_KEYWORDS = {
        'AI': ['äººå·¥æ™ºèƒ½', 'AI', 'å¤§æ¨¡å‹', 'ç®—åŠ›', 'ChatGPT'],
        'äº‘è®¡ç®—': ['äº‘è®¡ç®—', 'äº‘æœåŠ¡', 'æ•°æ®ä¸­å¿ƒ', 'æœåŠ¡å™¨'],
        'å¤§æ•°æ®': ['å¤§æ•°æ®', 'æ•°æ®è¦ç´ ', 'æ•°æ®èµ„äº§'],
        'èŠ¯ç‰‡': ['èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'æ™¶åœ†'],
        'é‡‘èç§‘æŠ€': ['é‡‘èç§‘æŠ€', 'æ•°å­—é‡‘è', 'FinTech', 'é‡‘èAI'],
        'é«˜è‚¡æ¯': ['é«˜è‚¡æ¯', 'çº¢åˆ©', 'åˆ†çº¢', 'è‚¡æ¯ç‡'],
        'åŒ»è¯': ['åŒ»è¯', 'åˆ›æ–°è¯', 'åŒ»ç–—å™¨æ¢°', 'ç”Ÿç‰©åŒ»è¯'],
        'æ–°èƒ½æº': ['æ–°èƒ½æº', 'å…‰ä¼', 'é£ç”µ', 'å‚¨èƒ½'],
        'æ±½è½¦': ['æ±½è½¦', 'æ–°èƒ½æºæ±½è½¦', 'æ™ºèƒ½é©¾é©¶', 'è½¦è”ç½‘'],
        'æ¸¸æˆ': ['æ¸¸æˆ', 'ç”µç«', 'å…ƒå®‡å®™', 'æ¸¸æˆAI']
    }
    
    # åŸºé‡‘ä»£ç æ˜ å°„
    FUND_CODES = {
        'AI': ['äººå·¥æ™ºèƒ½AI ETF(515070)', 'AIäººå·¥æ™ºèƒ½ETF(512930)'],
        'äº‘è®¡ç®—': ['äº‘è®¡ç®—ETF(516510)', 'å¤§æ•°æ®äº§ä¸šETF(516700)'],
        'å¤§æ•°æ®': ['å¤§æ•°æ®ETF(515400)', 'æ•°æ®ETF(515050)'],
        'èŠ¯ç‰‡': ['èŠ¯ç‰‡ETF(512760)', 'åŠå¯¼ä½“ETF(512480)'],
        'é‡‘èç§‘æŠ€': ['é‡‘èç§‘æŠ€ETF(159851)', 'è¯åˆ¸ETF(512880)'],
        'é«˜è‚¡æ¯': ['çº¢åˆ©ä½æ³¢ETF(512890)', 'å¤®ä¼çº¢åˆ©ETF(561580)'],
        'åŒ»è¯': ['åŒ»ç–—ETF(512170)', 'åˆ›æ–°è¯ETF(159992)'],
        'æ–°èƒ½æº': ['æ–°èƒ½æºæ±½è½¦ETF(515030)', 'å…‰ä¼ETF(515790)'],
        'æ±½è½¦': ['æ™ºèƒ½é©¾é©¶ETF(516520)', 'æ±½è½¦ETF(516110)'],
        'æ¸¸æˆ': ['æ¸¸æˆETF(159869)', 'ä¼ åª’ETF(512980)']
    }
    
    # å›¾æ ‡æ˜ å°„
    ICONS = ['ğŸª™', 'ğŸ¤–', 'ğŸ“Š', 'ğŸ’¡', 'ğŸ”¬', 'ğŸš€', 'ğŸ’¹', 'ğŸ“ˆ', 'ğŸ¯', 'ğŸ†', 'ğŸŒŸ', 'âš¡']
    
    @classmethod
    def process_news(cls, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¤„ç†æ–°é—»æ•°æ®ï¼Œæå–æ ¸å¿ƒä¿¡æ¯å¹¶å…³è”åŸºé‡‘ï¼Œç¡®ä¿ç”Ÿæˆè‡³å°‘10æ¡
        ç°åœ¨æ‰€æœ‰æ–°é—»éƒ½æ˜¯ä»ç½‘ç«™çˆ¬å–çš„ï¼Œæ ‡è®°ä¸ºæ–°æ–°é—»
        è¦æ±‚ï¼š1) 10æ¡æ–°é—»çš„å…³è”åŸºé‡‘ä¸é‡å¤ï¼›2) é‡å¤ä¿¡æ¯æ±‡æ€»æˆ1æ¡ï¼›3) ä¸è¶³10æ¡æ—¶ä»çˆ¬å–æ•°æ®ä¸­è¡¥å……
        """
        # ç¬¬ä¸€æ­¥ï¼šå»é‡ï¼Œå°†å†…å®¹é‡å¤çš„æ–°é—»åˆå¹¶
        unique_news = []
        seen_content = set()
        
        for news in news_list:
            title = news['title']
            detail = news.get('detail', '')
            
            # ç”Ÿæˆæ–°é—»å†…å®¹çš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨äºå»é‡
            content_key = title[:50] + '|' + detail[:100] if detail else title
            
            if content_key not in seen_content:
                seen_content.add(content_key)
                unique_news.append(news)
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ–°é—»ï¼Œç¡®ä¿å…³è”åŸºé‡‘ä¸é‡å¤
        processed_news = []
        used_funds = set()
        
        for news in unique_news:
            title = news['title']
            detail = news.get('detail', '')
            
            # å…³è”åŸºé‡‘
            related_funds = cls._get_related_funds(title + detail)
            
            # æ£€æŸ¥å…³è”åŸºé‡‘æ˜¯å¦å·²è¢«ä½¿ç”¨
            fund_key = '|'.join(related_funds)
            if fund_key not in used_funds:
                # æœªä½¿ç”¨è¿‡çš„åŸºé‡‘ç»„åˆï¼Œæ·»åŠ åˆ°ç»“æœä¸­
                used_funds.add(fund_key)
                
                processed_news.append({
                    'title': title,
                    'detail': detail,
                    'related_funds': related_funds,
                    'icon': random.choice(cls.ICONS),
                    'source': news['source'],
                    'is_new': True  # æ‰€æœ‰çˆ¬å–çš„æ–°é—»éƒ½æ ‡è®°ä¸ºæ–°æ–°é—»
                })
            
            # é™åˆ¶æœ€å¤šå¤„ç†20æ¡ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„é€‰æ‹©ç©ºé—´
            if len(processed_news) >= 20:
                break
        
        # ç¬¬ä¸‰æ­¥ï¼šç¡®ä¿è¿”å›10æ¡æ–°é—»
        if len(processed_news) < 10:
            # å¦‚æœä¸è¶³10æ¡ï¼Œä»å‰©ä½™çš„unique_newsä¸­è¡¥å……
            # å…ˆæ”¶é›†å·²ä½¿ç”¨çš„åŸºé‡‘ç»„åˆ
            used_fund_combinations = {tuple(news['related_funds']) for news in processed_news}
            
            for news in unique_news:
                title = news['title']
                detail = news.get('detail', '')
                
                # å…³è”åŸºé‡‘
                related_funds = cls._get_related_funds(title + detail)
                
                # æ£€æŸ¥åŸºé‡‘ç»„åˆæ˜¯å¦å·²ä½¿ç”¨
                if tuple(related_funds) not in used_fund_combinations:
                    # å¦‚æœåŸºé‡‘ç»„åˆæœªä½¿ç”¨ï¼Œç›´æ¥æ·»åŠ 
                    processed_news.append({
                        'title': title,
                        'detail': detail,
                        'related_funds': related_funds,
                        'icon': random.choice(cls.ICONS),
                        'source': news['source'],
                        'is_new': True
                    })
                    used_fund_combinations.add(tuple(related_funds))
                else:
                    # å¦‚æœåŸºé‡‘ç»„åˆå·²ä½¿ç”¨ï¼Œå°è¯•ç”Ÿæˆæ–°çš„åŸºé‡‘ç»„åˆ
                    new_funds = cls._get_related_funds(title + detail, avoid_funds=used_funds)
                    if new_funds and tuple(new_funds) not in used_fund_combinations:
                        processed_news.append({
                            'title': title,
                            'detail': detail,
                            'related_funds': new_funds,
                            'icon': random.choice(cls.ICONS),
                            'source': news['source'],
                            'is_new': True
                        })
                        used_fund_combinations.add(tuple(new_funds))
                
                if len(processed_news) >= 10:
                    break
        
        return processed_news[:10]
    
    @classmethod
    def _get_related_funds(cls, text: str, avoid_funds: set = None) -> List[str]:
        """
        æ ¹æ®æ–°é—»å†…å®¹è·å–å…³è”åŸºé‡‘ï¼Œç¡®ä¿ä¸æ–°é—»ä¸»é¢˜ç›¸å…³
        å‚æ•°ï¼š
            text: æ–°é—»å†…å®¹
            avoid_funds: å·²ä½¿ç”¨çš„åŸºé‡‘ç»„åˆé›†åˆï¼Œç”¨äºé¿å…é‡å¤
        """
        if avoid_funds is None:
            avoid_funds = set()
        
        # æ‰€æœ‰å¯èƒ½çš„åŸºé‡‘ç»„åˆ
        all_possible_funds = []
        
        # 1. ä¼˜å…ˆåŒ¹é…æœ€ç›¸å…³çš„åŸºé‡‘ç±»å‹
        for fund_type, keywords in cls.FUND_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text:
                    # æ·»åŠ è¯¥ç±»å‹çš„åŸºé‡‘
                    funds = cls.FUND_CODES.get(fund_type, [])
                    if funds:
                        all_possible_funds.append(funds[:2])
                    break
        
        # 2. å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œæ ¹æ®æ–°é—»å†…å®¹ä¸­çš„å…·ä½“å…³é”®è¯åŒ¹é…
        if not all_possible_funds:
            # é’ˆå¯¹ç‰¹å®šä¸»é¢˜çš„åŸºé‡‘åŒ¹é…
            if 'REITs' in text or 'ä¿ç§Ÿæˆ¿' in text or 'ä¸åŠ¨äº§' in text:
                all_possible_funds.append(['åŸºç¡€è®¾æ–½REITs', 'ä¿åˆ©å‘å±•REIT'])
            elif 'æ¸¯è‚¡' in text or 'é¦™æ¸¯' in text:
                all_possible_funds.append(['æ’ç”ŸETF(159920)', 'æ¸¯è‚¡é€šETF(513550)'])
            elif 'åŸºé‡‘å…¬å¸' in text or 'è‚¡æƒ' in text or 'è½¬è®©' in text:
                all_possible_funds.append(['åŸºé‡‘æŒ‡æ•°ETF', 'é‡‘èETF(512070)'])
            elif 'Aè‚¡' in text or 'è‚¡å¸‚' in text:
                all_possible_funds.append(['æ²ªæ·±300ETF(510300)', 'ä¸­è¯500ETF(510500)'])
            elif 'ETF' in text:
                all_possible_funds.append(['ETFåŸºé‡‘(510050)', 'ç§‘æŠ€ETF(515000)'])
            elif 'æ¶ˆè´¹' in text:
                all_possible_funds.append(['æ¶ˆè´¹ETF(510150)', 'ç™½é…’ETF(512690)'])
            elif 'åŒ»è¯' in text:
                all_possible_funds.append(['åŒ»ç–—ETF(512170)', 'åˆ›æ–°è¯ETF(159992)'])
            elif 'æ–°èƒ½æº' in text or 'å…‰ä¼' in text or 'é£ç”µ' in text:
                all_possible_funds.append(['æ–°èƒ½æºETF(516160)', 'å…‰ä¼ETF(515790)'])
            elif 'ç§‘æŠ€' in text or 'äººå·¥æ™ºèƒ½' in text:
                all_possible_funds.append(['ç§‘æŠ€ETF(515000)', 'äººå·¥æ™ºèƒ½ETF(515070)'])
            else:
                # é»˜è®¤åŸºé‡‘ï¼Œä¸æ–°é—»ä¸»é¢˜ç›¸å…³
                all_possible_funds.append(['ç»¼åˆæŒ‡æ•°ETF(510300)', 'æ··åˆåŸºé‡‘'])
        
        # 3. æ·»åŠ æ›´å¤šå¯èƒ½çš„åŸºé‡‘ç»„åˆï¼Œå¢åŠ å¤šæ ·æ€§
        additional_funds = [
            ['èŠ¯ç‰‡ETF(512760)', 'åŠå¯¼ä½“ETF(512480)'],
            ['é‡‘èç§‘æŠ€ETF(159851)', 'è¯åˆ¸ETF(512880)'],
            ['çº¢åˆ©ä½æ³¢ETF(512890)', 'å¤®ä¼çº¢åˆ©ETF(561580)'],
            ['æ–°èƒ½æºæ±½è½¦ETF(515030)', 'å…‰ä¼ETF(515790)'],
            ['æ™ºèƒ½é©¾é©¶ETF(516520)', 'æ±½è½¦ETF(516110)'],
            ['æ¸¸æˆETF(159869)', 'ä¼ åª’ETF(512980)'],
            ['äº‘è®¡ç®—ETF(516510)', 'å¤§æ•°æ®äº§ä¸šETF(516700)'],
            ['å¤§æ•°æ®ETF(515400)', 'æ•°æ®ETF(515050)']
        ]
        
        all_possible_funds.extend(additional_funds)
        
        # 4. ç­›é€‰å‡ºæœªä½¿ç”¨è¿‡çš„åŸºé‡‘ç»„åˆ
        for funds in all_possible_funds:
            fund_key = '|'.join(funds)
            if fund_key not in avoid_funds:
                return funds
        
        # 5. å¦‚æœæ‰€æœ‰åŸºé‡‘ç»„åˆéƒ½å·²ä½¿ç”¨ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„åŸºé‡‘ç»„åˆ
        # ä»æ‰€æœ‰åŸºé‡‘ä¸­éšæœºé€‰æ‹©2ä¸ªä¸åŒçš„åŸºé‡‘
        all_funds = []
        for funds in cls.FUND_CODES.values():
            all_funds.extend(funds)
        
        # æ·»åŠ ä¸€äº›ç‰¹æ®ŠåŸºé‡‘
        all_funds.extend(['åŸºç¡€è®¾æ–½REITs', 'ä¿åˆ©å‘å±•REIT', 'æ’ç”ŸETF(159920)', 'æ¸¯è‚¡é€šETF(513550)'])
        
        # å»é‡
        all_funds = list(set(all_funds))
        
        # ç”Ÿæˆæ–°çš„åŸºé‡‘ç»„åˆ
        import random
        for i in range(10):  # å°è¯•10æ¬¡
            new_funds = random.sample(all_funds, min(2, len(all_funds)))
            fund_key = '|'.join(new_funds)
            if fund_key not in avoid_funds:
                return new_funds
        
        # 6. å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤åŸºé‡‘
        return ['ç»¼åˆæŒ‡æ•°ETF(510300)', 'æ··åˆåŸºé‡‘']
    
    @classmethod
    def generate_core_tip(cls, news_list: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆæ ¸å¿ƒæç¤ºï¼Œç¡®ä¿è‡³å°‘200å­—ï¼Œèšç„¦äºåŸºé‡‘ç›¸å…³ä¿¡æ¯
        """
        # ç»Ÿè®¡åŸºé‡‘ç›¸å…³å…³é”®è¯
        keyword_count = {}
        fund_mentions = {}
        
        for news in news_list:
            title = news['title'] + news['detail']
            
            # ç»Ÿè®¡åŸºé‡‘ç±»å‹å…³é”®è¯
            for fund_type, keywords in cls.FUND_KEYWORDS.items():
                for keyword in keywords:
                    if keyword in title:
                        keyword_count[fund_type] = keyword_count.get(fund_type, 0) + 1
            
            # ç»Ÿè®¡åŸºé‡‘åç§°æåŠ
            for fund_type, funds in cls.FUND_CODES.items():
                for fund in funds:
                    if fund in title or fund.split('(')[0] in title:
                        fund_mentions[fund] = fund_mentions.get(fund, 0) + 1
        
        # è·å–çƒ­é—¨åŸºé‡‘ç±»å‹
        popular_fund_types = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # è·å–è¢«æåŠæœ€å¤šçš„åŸºé‡‘
        popular_funds = sorted(fund_mentions.items(), key=lambda x: x[1], reverse=True)[:2]
        
        # æ„å»ºåŸºç¡€æ ¸å¿ƒæç¤ºï¼Œèšç„¦äºåŸºé‡‘ç›¸å…³ä¿¡æ¯
        base_tip = "ä»Šæ—¥åŸºé‡‘å¸‚åœºå…³æ³¨ç‚¹èšç„¦äº"
        if popular_fund_types:
            popular_types_str = ', '.join([ft[0] for ft in popular_fund_types])
            base_tip += f"{popular_types_str}ç­‰åŸºé‡‘ç±»å‹ï¼Œ"
        else:
            base_tip += "å®è§‚ç»æµæ•°æ®ã€è¡Œä¸šæ”¿ç­–å¯¹åŸºé‡‘å¸‚åœºçš„å½±å“ï¼Œ"
        
        # æ‰©å±•æ ¸å¿ƒæç¤ºï¼Œç¡®ä¿è‡³å°‘200å­—ï¼Œå¹¶ä¸”æ¯å¥æ¢è¡Œåˆ†æ®µ
        # åœ¨HTMLä¸­ä½¿ç”¨<br/><br/>å®ç°çœŸæ­£çš„æ¢è¡Œåˆ†æ®µ
        core_tip = f"{base_tip}åŸºé‡‘å¸‚åœºæ•´ä½“è¡¨ç°æ´»è·ƒã€‚<br/><br/>"
        
        # åŠ å…¥çƒ­é—¨åŸºé‡‘ç±»å‹çš„å…·ä½“è¡¨ç°
        if popular_fund_types:
            for ft in popular_fund_types:
                fund_type = ft[0]
                if fund_type == 'AI':
                    core_tip += "AIç›¸å…³åŸºé‡‘è¡¨ç°å¼ºåŠ¿ï¼Œå¸‚åœºå¯¹äººå·¥æ™ºèƒ½é¢†åŸŸçš„æŠ•èµ„çƒ­æƒ…æŒç»­é«˜æ¶¨ã€‚<br/><br/>"
                elif fund_type == 'æ–°èƒ½æº':
                    core_tip += "æ–°èƒ½æºåŸºé‡‘å»¶ç»­ä¸Šæ¶¨æ€åŠ¿ï¼Œå…‰ä¼ã€é£ç”µç­‰ç»†åˆ†é¢†åŸŸæ¶¨å¹…æ˜¾è‘—ã€‚<br/><br/>"
                elif fund_type == 'åŒ»è¯':
                    core_tip += "åŒ»è¯åŸºé‡‘è¡¨ç°æ´»è·ƒï¼Œåˆ›æ–°è¯ã€åŒ»ç–—å™¨æ¢°ç­‰ç»†åˆ†é¢†åŸŸå—åˆ°å¸‚åœºå…³æ³¨ã€‚<br/><br/>"
                elif fund_type == 'èŠ¯ç‰‡':
                    core_tip += "èŠ¯ç‰‡åŸºé‡‘éœ‡è¡ä¸Šè¡Œï¼ŒåŠå¯¼ä½“äº§ä¸šå‡çº§å¸¦æ¥çš„æŠ•èµ„æœºä¼šå—åˆ°é‡è§†ã€‚<br/><br/>"
                elif fund_type == 'é«˜è‚¡æ¯':
                    core_tip += "é«˜è‚¡æ¯åŸºé‡‘é˜²å¾¡æ€§ä¼˜åŠ¿å‡¸æ˜¾ï¼Œæˆä¸ºå¸‚åœºæ³¢åŠ¨ä¸­çš„ç¨³å¥é€‰æ‹©ã€‚<br/><br/>"
                else:
                    core_tip += f"{fund_type}ç›¸å…³åŸºé‡‘è¡¨ç°æ´»è·ƒï¼Œå¸å¼•èµ„é‡‘å…³æ³¨ã€‚<br/><br/>"
        
        # åŠ å…¥åŸºé‡‘å¸‚åœºæ•´ä½“æƒ…å†µ
        core_tip += "ETFå¸‚åœºäº¤æ˜“æ´»è·ƒï¼Œå¤šåªETFä»½é¢å‡ºç°æ˜æ˜¾å¢é•¿ã€‚<br/><br/>"
        
        # åŠ å…¥åŸºé‡‘å…¬å¸åŠ¨æ€
        core_tip += "åŸºé‡‘å…¬å¸æ–¹é¢ï¼Œå¤šå®¶æœºæ„å‘å¸ƒ2026å¹´æŠ•èµ„ç­–ç•¥ï¼Œçœ‹å¥½ç§‘æŠ€ã€æ¶ˆè´¹ç­‰é¢†åŸŸçš„æŠ•èµ„æœºä¼šã€‚<br/><br/>"
        
        # åŠ å…¥æŠ•èµ„å»ºè®®
        core_tip += "å±•æœ›åå¸‚ï¼Œå»ºè®®æŠ•èµ„è€…å…³æ³¨æ”¿ç­–åˆ©å¥½çš„åŸºé‡‘æ¿å—ï¼ŒæŠŠæ¡ç»“æ„æ€§æŠ•èµ„æœºä¼šï¼ŒåŒæ—¶æ³¨æ„æ§åˆ¶é£é™©ï¼Œæ ¹æ®è‡ªèº«é£é™©åå¥½åˆç†é…ç½®åŸºé‡‘èµ„äº§ã€‚"
        
        # ç¡®ä¿æ ¸å¿ƒæç¤ºè‡³å°‘200å­—
        if len(core_tip) < 200:
            core_tip += " æŠ•èµ„è€…å¯å…³æ³¨åŸºé‡‘å…¬å¸çš„æœ€æ–°åŠ¨æ€å’Œäº§å“å¸ƒå±€ï¼Œé€‰æ‹©æŠ•èµ„ä¸šç»©ç¨³å®šã€ç®¡ç†èƒ½åŠ›å¼ºçš„åŸºé‡‘äº§å“ã€‚åœ¨å¸‚åœºæ³¢åŠ¨è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œåˆ†æ•£æŠ•èµ„ã€é•¿æœŸæŒæœ‰æ˜¯è¾ƒä¸ºç¨³å¥çš„æŠ•èµ„æ–¹å¼ã€‚"
        
        return core_tip


class NewsGenerator:
    """
    è´¢ç»æ–°é—»ç”Ÿæˆå™¨

    è´Ÿè´£å°†å¤„ç†åçš„æ–°é—»ç”Ÿæˆç¬¦åˆå…¬ä¼—å·æ¨¡æ¿çš„HTMLæ–‡ä»¶ã€‚
    """
    
    @classmethod
    def generate_html(cls, processed_news: List[Dict[str, Any]], core_tip: str) -> str:
        """
        ç”Ÿæˆé€‚åˆå¾®ä¿¡å…¬ä¼—å·çš„çº¯HTMLæ ¼å¼ï¼Œæ— CSSç±»å’Œæ ·å¼æ ‡ç­¾
        """
        # ç”Ÿæˆå½“å‰æ—¥æœŸ
        today = datetime.now().strftime('%Y-%m-%d')
        
        # æ„å»ºçº¯HTMLå†…å®¹ï¼Œä¸ä½¿ç”¨ä»»ä½•CSSç±»ï¼Œåªä½¿ç”¨å†…è”æ ·å¼
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è´¢ç»æ–°é—» - {today}</title>
</head>
<body>
    <!-- æ ¸å¿ƒæç¤º -->
    <p style="margin:20px 0; font-size:16px; line-height:2;"><strong style="font-size:16px; color:#333;">æ ¸å¿ƒæç¤º</strong>ï¼š{core_tip}</p>
    
    <!-- æ–°é—»åˆ—è¡¨ -->
    {cls._generate_news_items(processed_news)}
</body>
</html>
        """
        
        return html_content
    
    @classmethod
    def _generate_news_items(cls, processed_news: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆæ–°é—»æ¡ç›®ï¼Œä½¿ç”¨çº¯HTMLå’Œå†…è”æ ·å¼ï¼Œå…¼å®¹å¾®ä¿¡å…¬ä¼—å·ç¼–è¾‘å™¨
        è¦æ±‚ï¼š1.æ‰€æœ‰å­—ä½“å¤§å°16 2.æ¯æ¡é—´ç©º1è¡Œ 3.æ‘˜è¦å’Œå…³è”åŸºé‡‘åŠ ç²— 4.åŒºåˆ†æ–°æ—§æ–°é—»
        """
        news_items_html = ''
        
        for i, news in enumerate(processed_news, 1):
            icon = news['icon']
            title = news['title']
            detail = news['detail']
            funds = news['related_funds']
            is_new = news.get('is_new', True)  # æ˜¯å¦ä¸ºæ–°æ–°é—»
            
            # æ—§æ–°é—»æ ‡é¢˜åæ·»åŠ ğŸ”„å›¾æ ‡æ³¨æ˜
            if not is_new:
                title += ' ğŸ”„'
            
            # ç”Ÿæˆå…³è”åŸºé‡‘éƒ¨åˆ†
            funds_content = ''
            if funds:
                funds_content = 'ã€'.join(funds)
            else:
                # é‡æ–°è°ƒç”¨åŸºé‡‘åŒ¹é…æ–¹æ³•ï¼Œç¡®ä¿èƒ½åŒ¹é…åˆ°ç›¸å…³åŸºé‡‘
                funds = NewsProcessor._get_related_funds(title + detail)
                funds_content = 'ã€'.join(funds) if funds else 'æš‚æ— ç›¸å…³åŸºé‡‘'
            
            # ç”Ÿæˆçº¯HTMLå’Œå†…è”æ ·å¼çš„æ–°é—»æ¡ç›®ï¼Œç¬¦åˆæ‰€æœ‰è¦æ±‚
            news_items_html += f"""
    <!-- æ–°é—»æ¡ç›® {i} -->
    <p style="margin:20px 0 10px 0; font-size:16px; font-weight:bold; color:#333; line-height:1.8;">{icon}{i}. {title}</p>
    <p style="margin:10px 0; font-size:16px; color:#555; line-height:1.8;"><strong>æ‘˜è¦</strong>ï¼š{detail}</p>
    <p style="margin:10px 0 20px 0; font-size:16px; color:#27ae60; line-height:1.8;"><strong>å…³è”åŸºé‡‘</strong>ï¼š{funds_content}</p>
            """
        
        return news_items_html
    
    @classmethod
    def save_html(cls, html_content: str, output_dir: str = ".") -> str:
        """
        ä¿å­˜HTMLæ–‡ä»¶
        """
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(output_dir, "index.html")  # æ”¹ä¸ºindex.html
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return output_path


def main() -> None:
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œè´¢ç»æ–°é—»è·å–ã€å¤„ç†å’Œç”Ÿæˆçš„å®Œæ•´æµç¨‹
    """
    print("=" * 50)
    print("è´¢ç»æ–°é—»çˆ¬å–ç”Ÿæˆå·¥å…·")
    print("=" * 50)
    
    try:
        logger.info("åˆå§‹åŒ–æ–°é—»è·å–å™¨...")
        
        # 1. è·å–è´¢ç»æ–°é—»
        all_news = NewsFetcher.get_finance_news(count=50)
        
        if not all_news:
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„è´¢ç»æ–°é—»")
            return
        
        logger.info(f"è·å–åˆ° {len(all_news)} æ¡æ–°é—»")
        
        # 2. å¤„ç†æ–°é—»æ•°æ®
        processed_news = NewsProcessor.process_news(all_news)
        core_tip = NewsProcessor.generate_core_tip(processed_news)
        
        logger.info(f"å¤„ç†åç”Ÿæˆ {len(processed_news)} æ¡æ–°é—»")
        
        # 3. ç”ŸæˆHTML
        html_content = NewsGenerator.generate_html(processed_news, core_tip)
        
        # 4. ä¿å­˜HTMLæ–‡ä»¶
        output_path = NewsGenerator.save_html(html_content, output_dir=".")
        
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼Œç¡®ä¿æµè§ˆå™¨èƒ½æ­£ç¡®æ‰“å¼€
        import os
        absolute_path = os.path.abspath(output_path)
        
        print("\n" + "=" * 50)
        print("ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        print(f"è´¢ç»æ–°é—»å·²ä¿å­˜è‡³: {absolute_path}")
        print("æ­£åœ¨æ‰“å¼€æµè§ˆå™¨æŸ¥çœ‹...")
        print("=" * 50)
        
        # 5. è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨æŸ¥çœ‹ç”Ÿæˆçš„HTML
        import webbrowser
        # æ ¼å¼åŒ–Windowsè·¯å¾„ä¸ºfile://æ ¼å¼
        file_url = f"file:///{absolute_path.replace(chr(92), '/')}"
        webbrowser.open(file_url)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
